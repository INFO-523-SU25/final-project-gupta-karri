---
title: "Behavioral Outlier Segmentation using credit card dataset"
subtitle: "INFO 523 - Summer 2025 - Final Project"
author: "Saumya Gupta, Sathwika Karri"
title-slide-attributes:
  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
  
editor: visual
jupyter: python3
execute:
  echo: false
---

```{python}
#| label: load-packages
#| include: false

# Load packages here
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
import math
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
import numpy as np
from scipy import stats
from sklearn.cluster import (KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering)
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE

```

```{python}
#| label: setup
#| include: false
#| 
# Set up plot theme and figure resolution
sns.set_theme(style="whitegrid")
sns.set_context("notebook", font_scale=1.1)

import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.figsize'] = (6, 6 * 0.618)
```

```{python}
#| label: load-data Credit card and EDA 
#| include: false
# Load data in Python

def EDA_data(df):
    print("Basic Information:\n")
    print(df.info())
    print("\nShape of the dataset:\n", df.shape)
    print("\nData types:\n", df.dtypes)
    print("\nSummary of the dataset:\n", df.describe())
    print("\nFirst 5 rows:\n", df.head())
    print("\nLast 5 rows:\n", df.tail())

def missing_values(df):
    print("\nMissing values:\n", df.isnull().sum())

credit_card = pd.read_csv('data/CC GENERAL.csv')
EDA_data(credit_card)
missing_values(credit_card)


# Plotting with box-plot
def plot_boxplots(df, numerical_cols):
    n = len(numerical_cols)
    n_cols = 4
    n_rows = math.ceil(n / n_cols)
    plt.figure(figsize=(n_cols*5, n_rows*5))
    
    for i, col in enumerate(numerical_cols):
        plt.subplot(n_rows, n_cols , i+1)
        sns.boxplot(x=df[col])
    plt.show()
    
numerical_cols = credit_card.select_dtypes(include=['float64', 'int64']).columns.tolist()
plot_boxplots(credit_card, numerical_cols)


# Understanding the outliers
def num_outliers(df):
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

    count_outlier = {}
    
    for col in numerical_cols:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        IQR = q3 - q1
        lower_bound = q1 - 1.5 * IQR
        upper_bound = q3 + 1.5 * IQR   
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        count_outlier[col] = outliers.shape[0]
    
    outlier_df = pd.DataFrame(list(count_outlier.items()), columns=['Variable', 'Num_Outliers'])
    return outlier_df

outlier_counts_df = num_outliers(credit_card)
print(outlier_counts_df)

def plot_skewness(df):
    skew_values = df.skew(numeric_only=True)
    print("Skewness of numerical variables:\n", skew_values)

    num_cols = df.select_dtypes(include=['float64', 'int64']).columns

    plt.figure(figsize=(40, 40))
    for i, col in enumerate(num_cols, 1):
        plt.subplot(len(num_cols)//3 + 1, 3, i)  
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f"{col}\nSkewness: {skew_values[col]:.2f}")
    plt.show()

plot_skewness(credit_card)

```



```{python}
#| label: Preprocessing
#| include: false
# Load data in Python
def handling_missing_values(df):
    print("\n Missing values in credit card dataset: \n", df.isnull().sum())

handling_missing_values(credit_card)

def impute_missing_values(df):
    df_imputed = df.copy()
    credit_limit_median = df['CREDIT_LIMIT'].median()
    min_payments_median = df['MINIMUM_PAYMENTS'].median()
    
    return df.assign(
        CREDIT_LIMIT=df['CREDIT_LIMIT'].fillna(credit_limit_median),
        MINIMUM_PAYMENTS=df['MINIMUM_PAYMENTS'].fillna(min_payments_median)
    )

credit_df_imputed = impute_missing_values(credit_card)
print(credit_df_imputed.info())


def skewness_transformation(df, skew_threshold=1.0):
    df_trans = df.copy()
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    
    skew_before = df[numeric_cols].skew()
    
    for col in numeric_cols:
        data = df_trans[col]
        skew = skew_before[col]
        
        if abs(skew) <= skew_threshold:
            continue
            
        if skew > 3:
            try:
                if data.min() > 0:
                    trans_data, _ = stats.boxcox(data)
                else:
                    trans_data, _ = stats.yeojohnson(data)
                df_trans[col] = trans_data
            except:
                df_trans[col] = np.log1p(data - data.min())
                
        elif skew < -3:
            df_trans[col] = np.sign(data) * (np.abs(data) ** (1/3))
        else:
            if skew > 0:
                df_trans[col] = np.sqrt(data - data.min() + 1e-6)
            else:
                df_trans[col] = np.sign(data) * np.sqrt(np.abs(data))
    
    skew_after = df_trans[numeric_cols].skew()

    report = pd.DataFrame({
        'Before': skew_before,
        'After': skew_after,
        'Improvement': (skew_before.abs() - skew_after.abs())
    })
    
    return df_trans, report.sort_values('Improvement', ascending=False)

credit_df_transformed, skew_report = skewness_transformation(
    credit_df_imputed,
    skew_threshold=1.0
)
print("Skewness Transformation Report:")
display(skew_report)
print(credit_df_transformed.info())



def data_scaling(df, standard_cols=None, robust_cols=None, minmax_cols=None):
    if standard_cols is None:
        standard_cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 
                         'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_TRX',
                         'PAYMENTS', 'MINIMUM_PAYMENTS', 'ONEOFF_PURCHASES_FREQUENCY',
                         'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',
                         'CASH_ADVANCE_TRX', 'CREDIT_LIMIT']
    if robust_cols is None:
        robust_cols = ['BALANCE_FREQUENCY', 'TENURE']
    if minmax_cols is None:
        minmax_cols = ['PURCHASES_FREQUENCY']
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('std', StandardScaler(), standard_cols),
            ('robust', RobustScaler(), robust_cols),
            ('minmax', MinMaxScaler(), minmax_cols)
        ]
    )
    
    X_scaled = preprocessor.fit_transform(df)
    scaled_df = pd.DataFrame(X_scaled, columns=standard_cols + robust_cols + minmax_cols, index=df.index)
    return scaled_df

credit_df_scaled = data_scaling(credit_df_transformed)
print(credit_df_scaled.info())
```


```{python}
#| label: Feature_Engineering and Selection
#| include: false
def feature_engineering(df):
   df['PAYMENT_RATIO'] = df['PAYMENTS'] / (df['BALANCE'] + 1e-6) 
   df['MIN_PAYMENT_RATIO'] = df['MINIMUM_PAYMENTS'] / (df['BALANCE'] + 1e-6)
   df['ONEOFF_RATIO'] = df['ONEOFF_PURCHASES'] / (df['PURCHASES'] + 1e-6)
   df['INSTALLMENT_RATIO'] = df['INSTALLMENTS_PURCHASES'] / (df['PURCHASES'] + 1e-6)
   df['CREDIT_UTILIZATION'] = df['BALANCE'] / (df['CREDIT_LIMIT'] + 1e-6)
   df['CASH_ADVANCE_RATIO'] = df['CASH_ADVANCE'] / (df['PURCHASES'] + df['CASH_ADVANCE'] + 1e-6)
   df['PURCHASES_PER_TRX'] = df['PURCHASES'] / (df['PURCHASES_TRX'] + 1e-6)
   df['HIGH_CASH_ADVANCE'] = (df['CASH_ADVANCE'] > df['CASH_ADVANCE'].quantile(0.75)).astype(int)
   df['LOW_FREQUENCY'] = (df['PURCHASES_FREQUENCY'] < 0.2).astype(int)
   return df  

credit_df_featured = feature_engineering(credit_df_scaled)
print(credit_df_featured.info())

def feature_selection(df, corr_threshold=0.70):
    cluster_features = [
    # Based on Monetary
    'BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES','CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PAYMENT_RATIO', 
    'MIN_PAYMENT_RATIO', 'CREDIT_UTILIZATION', 'CASH_ADVANCE_RATIO',
    
    # Based on frequency 
    'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'PURCHASES_FREQUENCY','ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 
    'PURCHASES_PER_TRX', 'HIGH_CASH_ADVANCE','LOW_FREQUENCY',
    
    # Based on Recency
    'TENURE']

    df_selected = df[cluster_features].copy()
    # highly correlated values are removed
    corr_matrix = df_selected.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]
    df_selected.drop(columns=to_drop, inplace=True)
    
    print(f"Columns dropped due to high correlation (> {corr_threshold}): {to_drop}")
    print(f"Remaining columns using for clustering model: {df_selected.columns.tolist()}")
    
    return df_selected

feature_selected_clustering = feature_selection(credit_df_featured)
print(feature_selected_clustering.head())
print(feature_selected_clustering.info())


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_features = scaler.fit_transform(feature_selected_clustering)

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95) 
features_pca = pca.fit_transform(scaled_features)
print(f"Reduced to {features_pca.shape[1]} dimensions.")
```


```{python}
#| label: Silhouette score across different alg
#| include: false
from sklearn.cluster import (
    KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
)
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import pandas as pd

results = []

def evaluate_clustering(model, name, data):
    clusters = model.fit_predict(data)
    if len(set(clusters)) > 1: 
        score = silhouette_score(data, clusters)
        results.append({
            'Algorithm': name,
            'Silhouette Score': score,
            'Clusters': len(set(clusters)),
            'Noise Points': sum(clusters == -1) if hasattr(model, 'labels_') else 0
        })
        print(f"{name}: Score = {score:.3f}, Clusters = {len(set(clusters))}")
    else:
        print(f"{name}: Only 1 cluster detected.")

algorithms = {
    "KMeans (k=3)": KMeans(n_clusters=3, random_state=42),
    "GMM (k=3)": GaussianMixture(n_components=3, random_state=42),
    "Hierarchical (Ward)": AgglomerativeClustering(n_clusters=3, linkage='ward'),
    "DBSCAN (eps=0.5)": DBSCAN(eps=0.5, min_samples=5),
    "Spectral (k=3)": SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)
}

data = features_pca if 'features_pca' in locals() else scaled_features
for name, model in algorithms.items():
    evaluate_clustering(model, name, data)

results_df = pd.DataFrame(results)
print("\nClustering Performance Summary:")
print(results_df.sort_values('Silhouette Score', ascending=False))
```


```{python}
#| label: Performing k-means
#| include: false
inertia = []
k_range = range(2, 8)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(features_pca)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for K-means')
plt.xticks(k_range)
plt.show()

optimal_k = 4 
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(features_pca)

score = silhouette_score(features_pca, clusters)
print(f"K-means (k={optimal_k}) Silhouette Score: {score:.3f}")
```




```{python}
#| label: Labeling the clusters
#| include: false
X = feature_selected_clustering[['BALANCE', 'CREDIT_UTILIZATION', 'CASH_ADVANCE', 'PAYMENTS']]
kmeans = KMeans(n_clusters=4, random_state=42)
feature_selected_clustering['Cluster'] = kmeans.fit_predict(X)

cluster_means = feature_selected_clustering.groupby('Cluster')[
    ['BALANCE', 'CREDIT_UTILIZATION', 'CASH_ADVANCE', 'PAYMENTS']
].mean()

cluster_means['Risk_Score'] = (
    cluster_means['BALANCE'] * 0.3 +
    cluster_means['CREDIT_UTILIZATION'] * 0.4 +
    cluster_means['CASH_ADVANCE'] * 0.3 -
    cluster_means['PAYMENTS'] * 0.2
)

cluster_means = cluster_means.sort_values('Risk_Score')

risk_labels = ['Low Risk', 'Medium Risk', 'High Risk', 'Extreme Risk']

cluster_means['Risk_Label'] = risk_labels


cluster_risk_map = cluster_means['Risk_Label'].to_dict()

feature_selected_clustering['Risk_Label'] = feature_selected_clustering['Cluster'].map(cluster_risk_map)

print("Risk Label Distribution:")
print(feature_selected_clustering['Risk_Label'].value_counts().sort_index())
```

```{python}
#| label: Visualizing the clusters 
#| include: false
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
features_tsne = tsne.fit_transform(scaled_features)

plt.figure(figsize=(10, 6))
plt.scatter(features_tsne[:, 0], features_tsne[:, 1], 
            c=clusters, cmap='viridis', alpha=0.6, s=30)
plt.title("Cluster Visualization (t-SNE)")
plt.xlabel("t-SNE Dimension 1")
plt.ylabel("t-SNE Dimension 2")
plt.colorbar(label="Cluster")
plt.show()
```
---

## Objective 
- Group customers based on credit card spending, payment, and usage behavior  
- Identify customers likely to stop using their card and take proactive retention measures

---

## Business Problem 
1. Detect clusters of customers by transaction behavior (recency, frequency, monetary) and classify risk levels (high, medium, low) 

![credit card](img/credit.png){ width=300px }

2. Predict which customers might churn or switch to competitors

---

## Analytical Approach

- Using **cluster techniques** to group customers based on spending, payment frequency, and transaction history to identify common behavioral segments  
- Using **regression analysis** to predict the likelihood of customer churn for each individual based on behavioral patterns and segment  
- Tailor retention strategies based on behavioral segments

---

## Business Challenges

**Problem 1: We Don't Know Our Customers**

- Without behavioral segmentation, we treat all customers the same  
- High-risk users go unnoticed; best clients are not rewarded

**Problem 2: Customers Leave Without Warning**

- Customer churn is expensive: acquiring a new customer costs 5x more than retention  
- Need to predict potential churners proactively

---

## Work Flow

**EDA**
   ↓  
*Data Preprocessing*
   ↓  
**Feature Engineering & Selection**
   ↓  
**Clustering & Evaluation**  
   ↓  
**Select Best Clustering Algorithm**  
   ↓  
**Visualization of Clusters**
   ↓  
**Churn Prediction Modeling**  
   ↓  
**Feature Importance Analysis**  
   ↓  
**Business Insights & Recommendations**

---

## Outlier Detection

![](img/boxplots.png){ width=500px }

**Outlier Summary:**

- **BALANCE**: 695 outliers

- **PURCHASES**: 808 outliers 

- **CASH_ADVANCE**: 1,030 outliers

- **MINIMUM_PAYMENTS**: 841 outliers

## Distribution Analysis
![](img/skewness_histograms.png){ width=500px }

**Skewness Issues Identified:**

- **MINIMUM_PAYMENTS**: 13.62 (extremely skewed)

- **ONEOFF_PURCHASES**: 10.05 (highly skewed)

- **PURCHASES**: 8.14 (highly skewed)

## After Transformation
![](img/skewness.png){ width=500px }

- After applying transformations, the skewness of most numerical features was **significantly reduced**
- **Box-Cox** and **Yeo-Johnson** transformations applied for highly skewed features
- **Square root** and **cube root** transformations for moderately skewed features

---

## Summary of Cluster Algorithms
![](img/Summary.png){ width=500px }

 **K-Means (k=3) produced 3 clusters** with a moderate Silhouette Score (0.233), indicating some separation between clusters but not very strong.  

---

## Cluster Elbow Curve
![](img/Elbow.png){ width=500px }

- The Elbow Curve indicates that **4 clusters** capture most of the variation in the data.   
- Therefore, we divide customers into **4 behavioral segments** for further analysis.

---

## Clusters based on behavior 
![](img/cluster.png){ width=700px }

- **Cluster 0**: Low balance, low purchases, low risk
- **Cluster 1**: Medium balance, medium purchases, medium risk  
- **Cluster 2**: High balance, high purchases, high risk
- **Cluster 3**: Very high balance, very high purchases, extreme risk


---

## Customer Risk Label Distribution
![](img/Label.png){ width=500px }

- Most customers fall into **Low Risk (261)** and **Medium Risk (6,139)** categories.  
- A smaller number of customers are in **High Risk (2,453)** and **Extreme Risk (97)**

---

## Churn Prediction Analysis
## Feature Correlation Analysis
- **Highly correlated features (>0.8):**
  - PURCHASES ↔ ONEOFF_PURCHASES: 0.917
  - PURCHASES_FREQUENCY ↔ PURCHASES_INSTALLMENTS_FREQUENCY: 0.863
- These correlations help identify redundant features for model training

*Note: Correlation matrix heatmap would be generated during analysis*

## Churn Target Creation
- **Synthetic churn target** created using composite risk scoring
- **Churn rate: 25.01%** (2,238 out of 8,950 customers)

**Target Distribution:**
- **Non-Churn Customers**: 6,712 (74.99%)
- **Churn Customers**: 2,238 (25.01%)

**Risk factors considered:**
- Low purchase frequency
- High cash advance usage
- Irregular payment patterns
- High balance-to-credit ratio
- Risk indicators

## Model Evaluation Results

![](img/confusion_matrix.png){ width=600px }

| Metric | Score |
|--------|-------|
| **Test Set ROC-AUC** | 0.9994 |
| **Accuracy** | 98.94% |
| **Precision** | 98.42% |
| **Recall** | 97.32% |
| **F1-Score** | 97.87% |

## Acknowledgment

-   Thank you, Professor Greg Chism, for your guidance, support, and valuable feedback throughout this project.

-   Thank you to my classmates and team members for their collaboration, insights, and contributions.

```{python}
#| label: Correlation Analysis
#| include: false
# Correlation analysis for churn prediction
def correlation_analysis(df):
    """Analyze correlations between features and target"""
    print("=== CORRELATION ANALYSIS ===")
    
    # Select numerical columns for correlation
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    
    # Create correlation matrix
    corr_matrix = df[numerical_cols].corr()
    
    # Plot correlation heatmap
    plt.figure(figsize=(16, 12))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('Feature Correlation Matrix')
    plt.tight_layout()
    plt.savefig('img/correlation_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Find highly correlated features
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.8:
                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))
    
    if high_corr_pairs:
        print("\nHighly correlated feature pairs (>0.8):")
        for pair in high_corr_pairs:
            print(f"  {pair[0]} ↔ {pair[1]}: {pair[2]:.3f}")
    
    return corr_matrix

corr_matrix = correlation_analysis(credit_card)
```

```{python}
#| label: Churn Target Creation
#| include: false
def churn_feature_engineering(df):
    """Create features specifically for churn prediction"""
    # Create features with safe column access
    try:
        # Balance-to-credit-limit ratio
        df['BALANCE_CREDIT_RATIO'] = df['BALANCE'] / (df['CREDIT_LIMIT'] + 1e-6)
        
        # Payment-to-purchase ratio
        df['PAYMENT_PURCHASE_RATIO'] = df['PAYMENTS'] / (df['PURCHASES'] + 1)
        
        # Cash advance ratio
        df['CASH_ADVANCE_RATIO'] = df['CASH_ADVANCE'] / (df['BALANCE'] + 1)
        
        # Payment frequency score
        payment_score_components = [df['BALANCE_FREQUENCY'], df['PURCHASES_FREQUENCY']]
        df['PAYMENT_FREQUENCY_SCORE'] = sum(payment_score_components) / len(payment_score_components)
        
        # Spending behavior score
        spending_components = [df['PURCHASES_FREQUENCY'], df['ONEOFF_PURCHASES_FREQUENCY'], df['PURCHASES_INSTALLMENTS_FREQUENCY']]
        df['SPENDING_BEHAVIOR_SCORE'] = sum(spending_components) / len(spending_components)
        
        # Risk indicators
        df['HIGH_RISK_INDICATOR'] = (
            (df['CASH_ADVANCE'] > df['CASH_ADVANCE'].quantile(0.75)) |
            (df['BALANCE_CREDIT_RATIO'] > 0.8)
        ).astype(int)
        
        df['MEDIUM_RISK_INDICATOR'] = (
            (df['CASH_ADVANCE'].between(
                df['CASH_ADVANCE'].quantile(0.25), 
                df['CASH_ADVANCE'].quantile(0.75)
            )) |
            (df['BALANCE_CREDIT_RATIO'].between(0.4, 0.8))
        ).astype(int)
        
        return df
        
    except Exception as e:
        print(f"Error in feature engineering: {e}")
        return df

def create_churn_target(df):
    """Create synthetic target variable for churn prediction"""
    print("=== CREATING CHURN TARGET VARIABLE ===")
    
    # Calculate composite risk score
    risk_score = (
        # Low purchase frequency (negative impact)
        (0.3 - df['PURCHASES_FREQUENCY']).clip(lower=0) * 2 +
        
        # High cash advance usage (positive impact on churn risk)
        (df['CASH_ADVANCE_RATIO'] * 3) +
        
        # Irregular payment patterns (positive impact on churn risk)
        (1 - df['PAYMENT_FREQUENCY_SCORE']) * 2 +
        
        # High balance to credit ratio (positive impact on churn risk)
        (df['BALANCE_CREDIT_RATIO'] * 2) +
        
        # Low payment amounts relative to purchases (positive impact on churn risk)
        (1 - df['PAYMENT_PURCHASE_RATIO']).clip(lower=0) * 1.5 +
        
        # Risk indicators
        df['HIGH_RISK_INDICATOR'] * 3 +
        df['MEDIUM_RISK_INDICATOR'] * 1.5
    )
    
    # Normalize risk score to 0-1 range
    risk_score = (risk_score - risk_score.min()) / (risk_score.max() - risk_score.min())
    
    # Create binary churn target (1 = likely to churn, 0 = likely to stay)
    churn_threshold = risk_score.quantile(0.75)
    df['CHURN_TARGET'] = (risk_score > churn_threshold).astype(int)
    
    print(f"Churn target created:")
    print(f"  - Churn threshold: {churn_threshold:.3f}")
    print(f"  - Churn rate: {df['CHURN_TARGET'].mean():.2%}")
    print(f"  - Non-churn: {(1 - df['CHURN_TARGET']).sum()} customers")
    print(f"  - Churn: {df['CHURN_TARGET'].sum()} customers")
    
    return df

# Apply feature engineering and create churn target
credit_df_churn_features = churn_feature_engineering(credit_df_scaled)
credit_df_with_target = create_churn_target(credit_df_churn_features)
```

```{python}
#| label: Churn Model Training
#| include: false
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.pipeline import Pipeline

def churn_feature_selection(df, corr_threshold=0.85):
    """Select features for churn prediction model"""
    # Base features
    base_features = [
        'TENURE', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY',
        'PAYMENTS', 'MINIMUM_PAYMENTS', 'CASH_ADVANCE'
    ]
    
    # Add engineered features
    engineered_features = [
        'BALANCE_CREDIT_RATIO', 'PAYMENT_PURCHASE_RATIO', 'CASH_ADVANCE_RATIO',
        'PAYMENT_FREQUENCY_SCORE', 'SPENDING_BEHAVIOR_SCORE',
        'HIGH_RISK_INDICATOR', 'MEDIUM_RISK_INDICATOR'
    ]
    
    # Combine all features
    all_features = base_features + engineered_features
    
    # Check which features exist in the dataset
    available_features = [f for f in all_features if f in df.columns]
    
    # Select features and target
    X = df[available_features]
    y = df['CHURN_TARGET']
    
    print(f"Feature matrix shape: {X.shape}")
    print(f"Target distribution: {y.value_counts().to_dict()}")
    
    return X, y, available_features

X, y, feature_names = churn_feature_selection(credit_df_with_target)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")
print(f"Training churn rate: {y_train.mean():.2%}")
print(f"Testing churn rate: {y_test.mean():.2%}")

# Define models to try
models = {
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
}

# Train and evaluate models
best_score = 0
best_model_name = None
best_model = None

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    # Create pipeline with scaling
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', model)
    ])
    
    # Perform cross-validation
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')
    
    print(f"  Cross-validation ROC-AUC scores: {cv_scores}")
    print(f"  Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    if cv_scores.mean() > best_score:
        best_score = cv_scores.mean()
        best_model_name = name
        best_model = pipeline

print(f"\nBest model: {best_model_name} (CV ROC-AUC: {best_score:.4f})")

# Train the best model on full training data
best_model.fit(X_train, y_train)
```

```{python}
#| label: Model Evaluation and Feature Importance
#| include: false
# Evaluate the best model on the test set
print("=== MODEL EVALUATION ===")

# Make predictions
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Calculate metrics
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"Test Set Performance:")
print(f"  ROC-AUC Score: {roc_auc:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(f"\nConfusion Matrix:")
print(cm)

# Calculate additional metrics
tn, fp, fn, tp = cm.ravel()
accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"\nAdditional Metrics:")
print(f"  Accuracy: {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall: {recall:.4f}")
print(f"  F1-Score: {f1:.4f}")

# Analyze and display feature importance
print("=== FEATURE IMPORTANCE ANALYSIS ===")

# Get feature importance
if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):
    # Tree-based models
    importance = best_model.named_steps['classifier'].feature_importances_
elif hasattr(best_model.named_steps['classifier'], 'coef_'):
    # Linear models
    importance = np.abs(best_model.named_steps['classifier'].coef_[0])
else:
    print("Cannot extract feature importance from this model type.")

# Create feature importance dataframe
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance
}).sort_values('Importance', ascending=False)

print("Feature Importance (Top 10):")
print(feature_importance_df.head(10))

# Plot feature importance
plt.figure(figsize=(10, 8))
top_features = feature_importance_df.head(10)
plt.barh(range(len(top_features)), top_features['Importance'])
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Feature Importance')
plt.title('Top 10 Most Important Features for Churn Prediction')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('img/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
```
