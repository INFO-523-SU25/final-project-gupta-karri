[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "",
    "text": "Understanding the dataset structure\n\nCheck for missing values, data types, and distributions\nOutlier detection and treatment — since the dataset contains real transactional values, no outliers are removed\nIdentifying the skewness"
  },
  {
    "objectID": "main.html#step-1-exploratory-data-analysis-eda-1",
    "href": "main.html#step-1-exploratory-data-analysis-eda-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 1: Exploratory Data Analysis (EDA)",
    "text": "Step 1: Exploratory Data Analysis (EDA)\n\n# EDA \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\ndef EDA_data(df):\n    print(\"Basic Information:\\n\")\n    print(df.info())\n    print(\"\\nShape of the dataset:\\n\", df.shape)\n    print(\"\\nData types:\\n\", df.dtypes)\n    print(\"\\nSummary of the dataset:\\n\", df.describe())\n    print(\"\\nFirst 5 rows:\\n\", df.head())\n    print(\"\\nLast 5 rows:\\n\", df.tail())\n\ndef missing_values(df):\n    print(\"\\nMissing values:\\n\", df.isnull().sum())\n\ncredit_card = pd.read_csv('data/CC GENERAL.csv')\nEDA_data(credit_card)\nmissing_values(credit_card)\n\n\n# Plotting with box-plot\ndef plot_boxplots(df, numerical_cols):\n    n = len(numerical_cols)\n    n_cols = 4\n    n_rows = math.ceil(n / n_cols)\n    plt.figure(figsize=(n_cols*5, n_rows*5))\n    \n    for i, col in enumerate(numerical_cols):\n        plt.subplot(n_rows, n_cols , i+1)\n        sns.boxplot(x=df[col])\n    plt.savefig(\"img/boxplots.png\")\n    plt.show()\n    \nnumerical_cols = credit_card.select_dtypes(include=['float64', 'int64']).columns.tolist()\nplot_boxplots(credit_card, numerical_cols)\n\n\n# Understanding the outliers\ndef num_outliers(df):\n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n    count_outlier = {}\n    \n    for col in numerical_cols:\n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        IQR = q3 - q1\n        lower_bound = q1 - 1.5 * IQR\n        upper_bound = q3 + 1.5 * IQR   \n        outliers = df[(df[col] &lt; lower_bound) | (df[col] &gt; upper_bound)]\n        count_outlier[col] = outliers.shape[0]\n    \n    outlier_df = pd.DataFrame(list(count_outlier.items()), columns=['Variable', 'Num_Outliers'])\n    return outlier_df\n\noutlier_counts_df = num_outliers(credit_card)\nprint(outlier_counts_df)\n\ndef plot_skewness(df):\n    skew_values = df.skew(numeric_only=True)\n    print(\"Skewness of numerical variables:\\n\", skew_values)\n\n    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n\n    plt.figure(figsize=(40, 40))\n    for i, col in enumerate(num_cols, 1):\n        plt.subplot(len(num_cols)//3 + 1, 3, i)  \n        sns.histplot(df[col], kde=True, bins=30)\n        plt.title(f\"{col}\\nSkewness: {skew_values[col]:.2f}\")\n    plt.savefig(\"img/skewness_histograms.png\")\n    plt.show()\n\nplot_skewness(credit_card)\n\nBasic Information:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8949 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8637 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\n\nShape of the dataset:\n (8950, 18)\n\nData types:\n CUST_ID                              object\nBALANCE                             float64\nBALANCE_FREQUENCY                   float64\nPURCHASES                           float64\nONEOFF_PURCHASES                    float64\nINSTALLMENTS_PURCHASES              float64\nCASH_ADVANCE                        float64\nPURCHASES_FREQUENCY                 float64\nONEOFF_PURCHASES_FREQUENCY          float64\nPURCHASES_INSTALLMENTS_FREQUENCY    float64\nCASH_ADVANCE_FREQUENCY              float64\nCASH_ADVANCE_TRX                      int64\nPURCHASES_TRX                         int64\nCREDIT_LIMIT                        float64\nPAYMENTS                            float64\nMINIMUM_PAYMENTS                    float64\nPRC_FULL_PAYMENT                    float64\nTENURE                                int64\ndtype: object\n\nSummary of the dataset:\n             BALANCE  BALANCE_FREQUENCY     PURCHASES  ONEOFF_PURCHASES  \\\ncount   8950.000000        8950.000000   8950.000000       8950.000000   \nmean    1564.474828           0.877271   1003.204834        592.437371   \nstd     2081.531879           0.236904   2136.634782       1659.887917   \nmin        0.000000           0.000000      0.000000          0.000000   \n25%      128.281915           0.888889     39.635000          0.000000   \n50%      873.385231           1.000000    361.280000         38.000000   \n75%     2054.140036           1.000000   1110.130000        577.405000   \nmax    19043.138560           1.000000  49039.570000      40761.250000   \n\n       INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\ncount             8950.000000   8950.000000          8950.000000   \nmean               411.067645    978.871112             0.490351   \nstd                904.338115   2097.163877             0.401371   \nmin                  0.000000      0.000000             0.000000   \n25%                  0.000000      0.000000             0.083333   \n50%                 89.000000      0.000000             0.500000   \n75%                468.637500   1113.821139             0.916667   \nmax              22500.000000  47137.211760             1.000000   \n\n       ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\ncount                 8950.000000                       8950.000000   \nmean                     0.202458                          0.364437   \nstd                      0.298336                          0.397448   \nmin                      0.000000                          0.000000   \n25%                      0.000000                          0.000000   \n50%                      0.083333                          0.166667   \n75%                      0.300000                          0.750000   \nmax                      1.000000                          1.000000   \n\n       CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\ncount             8950.000000       8950.000000    8950.000000   8949.000000   \nmean                 0.135144          3.248827      14.709832   4494.449450   \nstd                  0.200121          6.824647      24.857649   3638.815725   \nmin                  0.000000          0.000000       0.000000     50.000000   \n25%                  0.000000          0.000000       1.000000   1600.000000   \n50%                  0.000000          0.000000       7.000000   3000.000000   \n75%                  0.222222          4.000000      17.000000   6500.000000   \nmax                  1.500000        123.000000     358.000000  30000.000000   \n\n           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT       TENURE  \ncount   8950.000000       8637.000000       8950.000000  8950.000000  \nmean    1733.143852        864.206542          0.153715    11.517318  \nstd     2895.063757       2372.446607          0.292499     1.338331  \nmin        0.000000          0.019163          0.000000     6.000000  \n25%      383.276166        169.123707          0.000000    12.000000  \n50%      856.901546        312.343947          0.000000    12.000000  \n75%     1901.134317        825.485459          0.142857    12.000000  \nmax    50721.483360      76406.207520          1.000000    12.000000  \n\nFirst 5 rows:\n   CUST_ID      BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n0  C10001    40.900749           0.818182      95.40              0.00   \n1  C10002  3202.467416           0.909091       0.00              0.00   \n2  C10003  2495.148862           1.000000     773.17            773.17   \n3  C10004  1666.670542           0.636364    1499.00           1499.00   \n4  C10005   817.714335           1.000000      16.00             16.00   \n\n   INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n0                    95.4      0.000000             0.166667   \n1                     0.0   6442.945483             0.000000   \n2                     0.0      0.000000             1.000000   \n3                     0.0    205.788017             0.083333   \n4                     0.0      0.000000             0.083333   \n\n   ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n0                    0.000000                          0.083333   \n1                    0.000000                          0.000000   \n2                    1.000000                          0.000000   \n3                    0.083333                          0.000000   \n4                    0.083333                          0.000000   \n\n   CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n0                0.000000                 0              2        1000.0   \n1                0.250000                 4              0        7000.0   \n2                0.000000                 0             12        7500.0   \n3                0.083333                 1              1        7500.0   \n4                0.000000                 0              1        1200.0   \n\n      PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n0   201.802084        139.509787          0.000000      12  \n1  4103.032597       1072.340217          0.222222      12  \n2   622.066742        627.284787          0.000000      12  \n3     0.000000               NaN          0.000000      12  \n4   678.334763        244.791237          0.000000      12  \n\nLast 5 rows:\n      CUST_ID     BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n8945  C19186   28.493517           1.000000     291.12              0.00   \n8946  C19187   19.183215           1.000000     300.00              0.00   \n8947  C19188   23.398673           0.833333     144.40              0.00   \n8948  C19189   13.457564           0.833333       0.00              0.00   \n8949  C19190  372.708075           0.666667    1093.25           1093.25   \n\n      INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n8945                  291.12      0.000000             1.000000   \n8946                  300.00      0.000000             1.000000   \n8947                  144.40      0.000000             0.833333   \n8948                    0.00     36.558778             0.000000   \n8949                    0.00    127.040008             0.666667   \n\n      ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n8945                    0.000000                          0.833333   \n8946                    0.000000                          0.833333   \n8947                    0.000000                          0.666667   \n8948                    0.000000                          0.000000   \n8949                    0.666667                          0.000000   \n\n      CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n8945                0.000000                 0              6        1000.0   \n8946                0.000000                 0              6        1000.0   \n8947                0.000000                 0              5        1000.0   \n8948                0.166667                 2              0         500.0   \n8949                0.333333                 2             23        1200.0   \n\n        PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n8945  325.594462         48.886365              0.50       6  \n8946  275.861322               NaN              0.00       6  \n8947   81.270775         82.418369              0.25       6  \n8948   52.549959         55.755628              0.25       6  \n8949   63.165404         88.288956              0.00       6  \n\nMissing values:\n CUST_ID                               0\nBALANCE                               0\nBALANCE_FREQUENCY                     0\nPURCHASES                             0\nONEOFF_PURCHASES                      0\nINSTALLMENTS_PURCHASES                0\nCASH_ADVANCE                          0\nPURCHASES_FREQUENCY                   0\nONEOFF_PURCHASES_FREQUENCY            0\nPURCHASES_INSTALLMENTS_FREQUENCY      0\nCASH_ADVANCE_FREQUENCY                0\nCASH_ADVANCE_TRX                      0\nPURCHASES_TRX                         0\nCREDIT_LIMIT                          1\nPAYMENTS                              0\nMINIMUM_PAYMENTS                    313\nPRC_FULL_PAYMENT                      0\nTENURE                                0\ndtype: int64\n\n\n\n\n\n\n\n\n\n                            Variable  Num_Outliers\n0                            BALANCE           695\n1                  BALANCE_FREQUENCY          1493\n2                          PURCHASES           808\n3                   ONEOFF_PURCHASES          1013\n4             INSTALLMENTS_PURCHASES           867\n5                       CASH_ADVANCE          1030\n6                PURCHASES_FREQUENCY             0\n7         ONEOFF_PURCHASES_FREQUENCY           782\n8   PURCHASES_INSTALLMENTS_FREQUENCY             0\n9             CASH_ADVANCE_FREQUENCY           525\n10                  CASH_ADVANCE_TRX           804\n11                     PURCHASES_TRX           766\n12                      CREDIT_LIMIT           248\n13                          PAYMENTS           808\n14                  MINIMUM_PAYMENTS           841\n15                  PRC_FULL_PAYMENT          1474\n16                            TENURE          1366\nSkewness of numerical variables:\n BALANCE                              2.393386\nBALANCE_FREQUENCY                   -2.023266\nPURCHASES                            8.144269\nONEOFF_PURCHASES                    10.045083\nINSTALLMENTS_PURCHASES               7.299120\nCASH_ADVANCE                         5.166609\nPURCHASES_FREQUENCY                  0.060164\nONEOFF_PURCHASES_FREQUENCY           1.535613\nPURCHASES_INSTALLMENTS_FREQUENCY     0.509201\nCASH_ADVANCE_FREQUENCY               1.828686\nCASH_ADVANCE_TRX                     5.721298\nPURCHASES_TRX                        4.630655\nCREDIT_LIMIT                         1.522464\nPAYMENTS                             5.907620\nMINIMUM_PAYMENTS                    13.622797\nPRC_FULL_PAYMENT                     1.942820\nTENURE                              -2.943017\ndtype: float64"
  },
  {
    "objectID": "main.html#step-2-data-preprocessing-1",
    "href": "main.html#step-2-data-preprocessing-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 2: Data Preprocessing",
    "text": "Step 2: Data Preprocessing\n\n# Preprocessing \nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\nfrom scipy import stats\n\ndef handling_missing_values(df):\n    print(\"\\n Missing values in credit card dataset: \\n\", df.isnull().sum())\n\nhandling_missing_values(credit_card)\n\ndef impute_missing_values(df):\n    df_imputed = df.copy()\n    credit_limit_median = df['CREDIT_LIMIT'].median()\n    min_payments_median = df['MINIMUM_PAYMENTS'].median()\n    \n    return df.assign(\n        CREDIT_LIMIT=df['CREDIT_LIMIT'].fillna(credit_limit_median),\n        MINIMUM_PAYMENTS=df['MINIMUM_PAYMENTS'].fillna(min_payments_median)\n    )\n\ncredit_df_imputed = impute_missing_values(credit_card)\nprint(credit_df_imputed.info())\n\n\ndef skewness_transformation(df, skew_threshold=1.0):\n    df_trans = df.copy()\n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n    \n    skew_before = df[numeric_cols].skew()\n    \n    for col in numeric_cols:\n        data = df_trans[col]\n        skew = skew_before[col]\n        \n        if abs(skew) &lt;= skew_threshold:\n            continue\n            \n        if skew &gt; 3:\n            try:\n                if data.min() &gt; 0:\n                    trans_data, _ = stats.boxcox(data)\n                else:\n                    trans_data, _ = stats.yeojohnson(data)\n                df_trans[col] = trans_data\n            except:\n                df_trans[col] = np.log1p(data - data.min())\n                \n        elif skew &lt; -3:\n            df_trans[col] = np.sign(data) * (np.abs(data) ** (1/3))\n        else:\n            if skew &gt; 0:\n                df_trans[col] = np.sqrt(data - data.min() + 1e-6)\n            else:\n                df_trans[col] = np.sign(data) * np.sqrt(np.abs(data))\n    \n    skew_after = df_trans[numeric_cols].skew()\n\n    report = pd.DataFrame({\n        'Before': skew_before,\n        'After': skew_after,\n        'Improvement': (skew_before.abs() - skew_after.abs())\n    })\n    \n    return df_trans, report.sort_values('Improvement', ascending=False)\n\ncredit_df_transformed, skew_report = skewness_transformation(\n    credit_df_imputed,\n    skew_threshold=1.0\n)\nprint(\"Skewness Transformation Report:\")\ndisplay(skew_report)\nprint(credit_df_transformed.info())\n\n\n\ndef data_scaling(df, standard_cols=None, robust_cols=None, minmax_cols=None):\n    if standard_cols is None:\n        standard_cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', \n                         'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_TRX',\n                         'PAYMENTS', 'MINIMUM_PAYMENTS', 'ONEOFF_PURCHASES_FREQUENCY',\n                         'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n                         'CASH_ADVANCE_TRX', 'CREDIT_LIMIT']\n    if robust_cols is None:\n        robust_cols = ['BALANCE_FREQUENCY', 'TENURE']\n    if minmax_cols is None:\n        minmax_cols = ['PURCHASES_FREQUENCY']\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('std', StandardScaler(), standard_cols),\n            ('robust', RobustScaler(), robust_cols),\n            ('minmax', MinMaxScaler(), minmax_cols)\n        ]\n    )\n    \n    X_scaled = preprocessor.fit_transform(df)\n    scaled_df = pd.DataFrame(X_scaled, columns=standard_cols + robust_cols + minmax_cols, index=df.index)\n    return scaled_df\n\ncredit_df_scaled = data_scaling(credit_df_transformed)\nprint(credit_df_scaled.info())\n\n\n Missing values in credit card dataset: \n CUST_ID                               0\nBALANCE                               0\nBALANCE_FREQUENCY                     0\nPURCHASES                             0\nONEOFF_PURCHASES                      0\nINSTALLMENTS_PURCHASES                0\nCASH_ADVANCE                          0\nPURCHASES_FREQUENCY                   0\nONEOFF_PURCHASES_FREQUENCY            0\nPURCHASES_INSTALLMENTS_FREQUENCY      0\nCASH_ADVANCE_FREQUENCY                0\nCASH_ADVANCE_TRX                      0\nPURCHASES_TRX                         0\nCREDIT_LIMIT                          1\nPAYMENTS                              0\nMINIMUM_PAYMENTS                    313\nPRC_FULL_PAYMENT                      0\nTENURE                                0\ndtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\nSkewness Transformation Report:\n\n\n\n\n\n\n\n\n\nBefore\nAfter\nImprovement\n\n\n\n\nMINIMUM_PAYMENTS\n13.852446\n-0.003489\n13.848957\n\n\nONEOFF_PURCHASES\n10.045083\n0.115147\n9.929936\n\n\nPURCHASES\n8.144269\n-0.178677\n7.965592\n\n\nINSTALLMENTS_PURCHASES\n7.299120\n-0.014843\n7.284277\n\n\nPAYMENTS\n5.907620\n0.124630\n5.782989\n\n\nCASH_ADVANCE_TRX\n5.721298\n0.392581\n5.328717\n\n\nCASH_ADVANCE\n5.166609\n0.188413\n4.978196\n\n\nPURCHASES_TRX\n4.630655\n0.006058\n4.624597\n\n\nBALANCE\n2.393386\n0.829500\n1.563886\n\n\nCASH_ADVANCE_FREQUENCY\n1.828686\n0.708929\n1.119757\n\n\nCREDIT_LIMIT\n1.522636\n0.669349\n0.853286\n\n\nONEOFF_PURCHASES_FREQUENCY\n1.535613\n0.726386\n0.809227\n\n\nPRC_FULL_PAYMENT\n1.942820\n1.298655\n0.644165\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n0.509201\n0.509201\n0.000000\n\n\nPURCHASES_FREQUENCY\n0.060164\n0.060164\n0.000000\n\n\nTENURE\n-2.943017\n-3.064332\n-0.121315\n\n\nBALANCE_FREQUENCY\n-2.023266\n-2.819495\n-0.796229\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  PURCHASES_TRX                     8950 non-null   float64\n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   float64\ndtypes: float64(17), object(1)\nmemory usage: 1.2+ MB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 16 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   BALANCE                           8950 non-null   float64\n 1   PURCHASES                         8950 non-null   float64\n 2   ONEOFF_PURCHASES                  8950 non-null   float64\n 3   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 4   CASH_ADVANCE                      8950 non-null   float64\n 5   PURCHASES_TRX                     8950 non-null   float64\n 6   PAYMENTS                          8950 non-null   float64\n 7   MINIMUM_PAYMENTS                  8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  CREDIT_LIMIT                      8950 non-null   float64\n 13  BALANCE_FREQUENCY                 8950 non-null   float64\n 14  TENURE                            8950 non-null   float64\n 15  PURCHASES_FREQUENCY               8950 non-null   float64\ndtypes: float64(16)\nmemory usage: 1.1 MB\nNone"
  },
  {
    "objectID": "main.html#step-3-feature-engineering-feature-selection-1",
    "href": "main.html#step-3-feature-engineering-feature-selection-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 3: Feature Engineering & Feature Selection",
    "text": "Step 3: Feature Engineering & Feature Selection\n\n# Feature Engineering, Feature Selection\n# In our dataset, all columns are numeric columns, so we are not performing encoding\n\ndef feature_engineering(df):\n   df['PAYMENT_RATIO'] = df['PAYMENTS'] / (df['BALANCE'] + 1e-6) \n   df['MIN_PAYMENT_RATIO'] = df['MINIMUM_PAYMENTS'] / (df['BALANCE'] + 1e-6)\n   df['ONEOFF_RATIO'] = df['ONEOFF_PURCHASES'] / (df['PURCHASES'] + 1e-6)\n   df['INSTALLMENT_RATIO'] = df['INSTALLMENTS_PURCHASES'] / (df['PURCHASES'] + 1e-6)\n   df['CREDIT_UTILIZATION'] = df['BALANCE'] / (df['CREDIT_LIMIT'] + 1e-6)\n   df['CASH_ADVANCE_RATIO'] = df['CASH_ADVANCE'] / (df['PURCHASES'] + df['CASH_ADVANCE'] + 1e-6)\n   df['PURCHASES_PER_TRX'] = df['PURCHASES'] / (df['PURCHASES_TRX'] + 1e-6)\n   df['HIGH_CASH_ADVANCE'] = (df['CASH_ADVANCE'] &gt; df['CASH_ADVANCE'].quantile(0.75)).astype(int)\n   df['LOW_FREQUENCY'] = (df['PURCHASES_FREQUENCY'] &lt; 0.2).astype(int)\n   return df  \n\ncredit_df_featured = feature_engineering(credit_df_scaled)\nprint(credit_df_featured.info())\n\ndef feature_selection(df, corr_threshold=0.70):\n    cluster_features = [\n    # Based on Monetary\n    'BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES','CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PAYMENT_RATIO', \n    'MIN_PAYMENT_RATIO', 'CREDIT_UTILIZATION', 'CASH_ADVANCE_RATIO',\n    \n    # Based on frequency \n    'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'PURCHASES_FREQUENCY','ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n    'PURCHASES_PER_TRX', 'HIGH_CASH_ADVANCE','LOW_FREQUENCY',\n    \n    # Based on Recency\n    'TENURE']\n\n    df_selected = df[cluster_features].copy()\n    # highly correlated values are removed\n    corr_matrix = df_selected.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    to_drop = [column for column in upper.columns if any(upper[column] &gt; corr_threshold)]\n    df_selected.drop(columns=to_drop, inplace=True)\n    \n    print(f\"Columns dropped due to high correlation (&gt; {corr_threshold}): {to_drop}\")\n    print(f\"Remaining columns using for clustering model: {df_selected.columns.tolist()}\")\n    \n    return df_selected\n\nfeature_selected_clustering = feature_selection(credit_df_featured)\nprint(feature_selected_clustering.head())\nprint(feature_selected_clustering.info())\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(feature_selected_clustering)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95) \nfeatures_pca = pca.fit_transform(scaled_features)\nprint(f\"Reduced to {features_pca.shape[1]} dimensions.\")\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 25 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   BALANCE                           8950 non-null   float64\n 1   PURCHASES                         8950 non-null   float64\n 2   ONEOFF_PURCHASES                  8950 non-null   float64\n 3   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 4   CASH_ADVANCE                      8950 non-null   float64\n 5   PURCHASES_TRX                     8950 non-null   float64\n 6   PAYMENTS                          8950 non-null   float64\n 7   MINIMUM_PAYMENTS                  8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  CREDIT_LIMIT                      8950 non-null   float64\n 13  BALANCE_FREQUENCY                 8950 non-null   float64\n 14  TENURE                            8950 non-null   float64\n 15  PURCHASES_FREQUENCY               8950 non-null   float64\n 16  PAYMENT_RATIO                     8950 non-null   float64\n 17  MIN_PAYMENT_RATIO                 8950 non-null   float64\n 18  ONEOFF_RATIO                      8950 non-null   float64\n 19  INSTALLMENT_RATIO                 8950 non-null   float64\n 20  CREDIT_UTILIZATION                8950 non-null   float64\n 21  CASH_ADVANCE_RATIO                8950 non-null   float64\n 22  PURCHASES_PER_TRX                 8950 non-null   float64\n 23  HIGH_CASH_ADVANCE                 8950 non-null   int32  \n 24  LOW_FREQUENCY                     8950 non-null   int32  \ndtypes: float64(23), int32(2)\nmemory usage: 1.6 MB\nNone\nColumns dropped due to high correlation (&gt; 0.7): ['ONEOFF_PURCHASES', 'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'HIGH_CASH_ADVANCE', 'LOW_FREQUENCY']\nRemaining columns using for clustering model: ['BALANCE', 'PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PAYMENT_RATIO', 'MIN_PAYMENT_RATIO', 'CREDIT_UTILIZATION', 'CASH_ADVANCE_RATIO', 'PURCHASES_PER_TRX', 'TENURE']\n    BALANCE  PURCHASES  INSTALLMENTS_PURCHASES  CASH_ADVANCE  CREDIT_LIMIT  \\\n0 -1.078055  -0.368050                0.383704     -0.944538     -1.216584   \n1  1.053619  -1.505149               -1.085422      1.399909      0.856426   \n2  0.771680   0.521657               -1.085422     -0.944538      0.972681   \n3  0.384093   0.875823               -1.085422      0.680451      0.972681   \n4 -0.135254  -0.904786               -1.085422     -0.944538     -1.094693   \n\n   PAYMENTS  PAYMENT_RATIO  MIN_PAYMENT_RATIO  CREDIT_UTILIZATION  \\\n0 -0.983876       0.912641           0.755774            0.886134   \n1  1.285321       1.219909           0.852837            1.230250   \n2 -0.291722      -0.378034           0.581978            0.793353   \n3 -2.782085      -7.243239          -0.355270            0.394880   \n4 -0.231669       1.712856           2.524060            0.123555   \n\n   CASH_ADVANCE_RATIO  PURCHASES_PER_TRX  TENURE  \n0            0.719600           0.641578     0.0  \n1          -13.302223           1.084696     0.0  \n2            2.233583           1.054367     0.0  \n3            0.437230          -1.003559     0.0  \n4            0.510748           1.036747     0.0  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 12 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   BALANCE                 8950 non-null   float64\n 1   PURCHASES               8950 non-null   float64\n 2   INSTALLMENTS_PURCHASES  8950 non-null   float64\n 3   CASH_ADVANCE            8950 non-null   float64\n 4   CREDIT_LIMIT            8950 non-null   float64\n 5   PAYMENTS                8950 non-null   float64\n 6   PAYMENT_RATIO           8950 non-null   float64\n 7   MIN_PAYMENT_RATIO       8950 non-null   float64\n 8   CREDIT_UTILIZATION      8950 non-null   float64\n 9   CASH_ADVANCE_RATIO      8950 non-null   float64\n 10  PURCHASES_PER_TRX       8950 non-null   float64\n 11  TENURE                  8950 non-null   float64\ndtypes: float64(12)\nmemory usage: 839.2 KB\nNone\nReduced to 10 dimensions."
  },
  {
    "objectID": "main.html#step-4-apply-clustering-algorithms-evaluate-1",
    "href": "main.html#step-4-apply-clustering-algorithms-evaluate-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 4: Apply Clustering Algorithms & Evaluate",
    "text": "Step 4: Apply Clustering Algorithms & Evaluate\n\n# Applying the clustering \n\nfrom sklearn.cluster import (\n    KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n)\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\nimport pandas as pd\n\nresults = []\n\ndef evaluate_clustering(model, name, data):\n    clusters = model.fit_predict(data)\n    if len(set(clusters)) &gt; 1: \n        score = silhouette_score(data, clusters)\n        results.append({\n            'Algorithm': name,\n            'Silhouette Score': score,\n            'Clusters': len(set(clusters)),\n            'Noise Points': sum(clusters == -1) if hasattr(model, 'labels_') else 0\n        })\n        print(f\"{name}: Score = {score:.3f}, Clusters = {len(set(clusters))}\")\n    else:\n        print(f\"{name}: Only 1 cluster detected.\")\n\nalgorithms = {\n    \"KMeans (k=3)\": KMeans(n_clusters=3, random_state=42),\n    \"GMM (k=3)\": GaussianMixture(n_components=3, random_state=42),\n    \"Hierarchical (Ward)\": AgglomerativeClustering(n_clusters=3, linkage='ward'),\n    \"DBSCAN (eps=0.5)\": DBSCAN(eps=0.5, min_samples=5),\n    \"Spectral (k=3)\": SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n}\n\ndata = features_pca if 'features_pca' in locals() else scaled_features\nfor name, model in algorithms.items():\n    evaluate_clustering(model, name, data)\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nClustering Performance Summary:\")\nprint(results_df.sort_values('Silhouette Score', ascending=False))\n\nKMeans (k=3): Score = 0.233, Clusters = 3\nGMM (k=3): Score = 0.026, Clusters = 3\nHierarchical (Ward): Score = 0.194, Clusters = 3\nDBSCAN (eps=0.5): Score = -0.340, Clusters = 58\nSpectral (k=3): Score = 0.143, Clusters = 3\n\nClustering Performance Summary:\n             Algorithm  Silhouette Score  Clusters  Noise Points\n0         KMeans (k=3)          0.232804         3             0\n2  Hierarchical (Ward)          0.193619         3             0\n4       Spectral (k=3)          0.142967         3             0\n1            GMM (k=3)          0.025639         3             0\n3     DBSCAN (eps=0.5)         -0.340413        58          3572"
  },
  {
    "objectID": "main.html#step-5-select-the-best-clustering-algorithm-1",
    "href": "main.html#step-5-select-the-best-clustering-algorithm-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 5: Select the Best Clustering Algorithm",
    "text": "Step 5: Select the Best Clustering Algorithm\n\nfrom sklearn.cluster import KMeans\n\ninertia = []\nk_range = range(2, 8)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(features_pca)\n    inertia.append(kmeans.inertia_)\n\nplt.figure(figsize=(8, 4))\nplt.plot(k_range, inertia, 'bo-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for K-means')\nplt.xticks(k_range)\nplt.savefig(\"img/Elbow.png\")\nplt.show()\n\noptimal_k = 4  \nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nclusters = kmeans.fit_predict(features_pca)\n\nscore = silhouette_score(features_pca, clusters)\nprint(f\"K-means (k={optimal_k}) Silhouette Score: {score:.3f}\")\n\n\n\n\n\n\n\n\nK-means (k=4) Silhouette Score: 0.227"
  },
  {
    "objectID": "main.html#step-6-visualization-of-clusters-1",
    "href": "main.html#step-6-visualization-of-clusters-1",
    "title": "Identifying Clustering Patterns Based on Transactional Behavior",
    "section": "Step 6: Visualization of Clusters",
    "text": "Step 6: Visualization of Clusters\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nfeatures_tsne = tsne.fit_transform(scaled_features)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(features_tsne[:, 0], features_tsne[:, 1], \n            c=clusters, cmap='viridis', alpha=0.6, s=30)\nplt.title(\"Cluster Visualization (t-SNE)\")\nplt.xlabel(\"t-SNE Dimension 1\")\nplt.ylabel(\"t-SNE Dimension 2\")\nplt.colorbar(label=\"Cluster\")\nplt.savefig(\"img/Clsuter.png\")\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by The Classifiers For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nSaumya Gupta: First-year Master of Science in Data Science student with a professional background in Software Development and interest in Machine Learning.\nSathwika Karri: I am in my fourth semester of the Master of Science in Data Science program."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "",
    "text": "The primary objective of this project was to analyze credit card transaction data to identify behavioral segments among customers and predict which customers are likely to churn. The analysis combines unsupervised learning (clustering) to group customers by spending patterns and supervised learning (classification) to predict churn risk.\nThe project addresses two critical business challenges: understanding customer behavior patterns and proactively identifying customers at risk of leaving. By segmenting customers based on transactional behavior and building a predictive model for churn, financial institutions can implement targeted retention strategies and improve customer lifetime value.\nThe analysis reveals that customers can be effectively grouped into four risk categories (Low, Medium, High, and Extreme Risk) based on their spending, payment, and credit utilization patterns. The churn prediction model achieves exceptional performance with 99.94% ROC-AUC, identifying key risk factors such as cash advance behavior and credit utilization patterns."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "",
    "text": "The primary objective of this project was to analyze credit card transaction data to identify behavioral segments among customers and predict which customers are likely to churn. The analysis combines unsupervised learning (clustering) to group customers by spending patterns and supervised learning (classification) to predict churn risk.\nThe project addresses two critical business challenges: understanding customer behavior patterns and proactively identifying customers at risk of leaving. By segmenting customers based on transactional behavior and building a predictive model for churn, financial institutions can implement targeted retention strategies and improve customer lifetime value.\nThe analysis reveals that customers can be effectively grouped into four risk categories (Low, Medium, High, and Extreme Risk) based on their spending, payment, and credit utilization patterns. The churn prediction model achieves exceptional performance with 99.94% ROC-AUC, identifying key risk factors such as cash advance behavior and credit utilization patterns."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Abstract",
    "text": "Abstract\nThis project leverages machine learning to segment credit card customers by behavioral patterns and predict customer churn risk. Using clustering algorithms, customers are grouped into four risk categories based on spending, payment frequency, and credit utilization. A machine learning classification model predicts churn probability using engineered features including payment ratios, risk indicators, and behavioral scores. The model achieves 99.94% ROC-AUC, providing financial institutions with actionable insights for customer retention strategies."
  },
  {
    "objectID": "index.html#question",
    "href": "index.html#question",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Question",
    "text": "Question\n\nGroup customers based on credit card spending, payment, and usage behavior\nIdentify customers likely to stop using their card and take proactive retention measures"
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Dataset",
    "text": "Dataset\nThe dataset contains credit card transaction data with 8,950 customers and 18 features including balance, purchases, cash advances, payment patterns, and credit utilization metrics. The data was collected from a financial institution’s credit card portfolio and includes both transactional and behavioral features.\n\n\nRows, Columns: (8950, 18)\n\n\n\n\n\n\n\n\n\n\nCUST_ID\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\n0\nC10001\n40.900749\n0.818182\n95.40\n0.00\n95.40\n0.000000\n0.166667\n0.000000\n0.083333\n0.000000\n0\n2\n1000.0\n201.802084\n139.509787\n0.000000\n12\n\n\n1\nC10002\n3202.467416\n0.909091\n0.00\n0.00\n0.00\n6442.945483\n0.000000\n0.000000\n0.000000\n0.250000\n4\n0\n7000.0\n4103.032597\n1072.340217\n0.222222\n12\n\n\n2\nC10003\n2495.148862\n1.000000\n773.17\n773.17\n0.00\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0\n12\n7500.0\n622.066742\n627.284787\n0.000000\n12\n\n\n3\nC10004\n1666.670542\n0.636364\n1499.00\n1499.00\n0.00\n205.788017\n0.083333\n0.083333\n0.000000\n0.083333\n1\n1\n7500.0\n0.000000\nNaN\n0.000000\n12\n\n\n4\nC10005\n817.714335\n1.000000\n16.00\n16.00\n0.00\n0.000000\n0.083333\n0.083333\n0.000000\n0.000000\n0\n1\n1200.0\n678.334763\n244.791237\n0.000000\n12\n\n\n5\nC10006\n1809.828751\n1.000000\n1333.28\n0.00\n1333.28\n0.000000\n0.666667\n0.000000\n0.583333\n0.000000\n0\n8\n1800.0\n1400.057770\n2407.246035\n0.000000\n12\n\n\n6\nC10007\n627.260806\n1.000000\n7091.01\n6402.63\n688.38\n0.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0\n64\n13500.0\n6354.314328\n198.065894\n1.000000\n12\n\n\n7\nC10008\n1823.652743\n1.000000\n436.20\n0.00\n436.20\n0.000000\n1.000000\n0.000000\n1.000000\n0.000000\n0\n12\n2300.0\n679.065082\n532.033990\n0.000000\n12\n\n\n8\nC10009\n1014.926473\n1.000000\n861.49\n661.49\n200.00\n0.000000\n0.333333\n0.083333\n0.250000\n0.000000\n0\n5\n7000.0\n688.278568\n311.963409\n0.000000\n12\n\n\n9\nC10010\n152.225975\n0.545455\n1281.60\n1281.60\n0.00\n0.000000\n0.166667\n0.166667\n0.000000\n0.000000\n0\n3\n11000.0\n1164.770591\n100.302262\n0.000000\n12"
  },
  {
    "objectID": "index.html#column-definitions",
    "href": "index.html#column-definitions",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Column Definitions",
    "text": "Column Definitions\n\nCUST_ID – Unique customer identifier\nBALANCE – Credit card balance amount\nBALANCE_FREQUENCY – Frequency of balance updates\nPURCHASES – Total purchase amount\nONEOFF_PURCHASES – One-time purchase amount\nINSTALLMENTS_PURCHASES – Installment purchase amount\nCASH_ADVANCE – Cash advance amount\nPURCHASES_FREQUENCY – Frequency of purchases\nONEOFF_PURCHASES_FREQUENCY – Frequency of one-time purchases\nPURCHASES_INSTALLMENTS_FREQUENCY – Frequency of installment purchases\nCASH_ADVANCE_FREQUENCY – Frequency of cash advances\nCASH_ADVANCE_TRX – Number of cash advance transactions\nPURCHASES_TRX – Number of purchase transactions\nCREDIT_LIMIT – Credit limit amount\nPAYMENTS – Payment amount\nMINIMUM_PAYMENTS – Minimum payment amount\nPRC_FULL_PAYMENT – Percentage of full payment\nTENURE – Length of customer relationship"
  },
  {
    "objectID": "index.html#eda-visualization",
    "href": "index.html#eda-visualization",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "EDA + Visualization",
    "text": "EDA + Visualization\n::: {#cell-distribution and outlier analysis .cell message=‘false’ execution_count=3}\n\n\n\n\n\n\n\n\n                            Variable  Num_Outliers\n0                            BALANCE           695\n1                  BALANCE_FREQUENCY          1493\n2                          PURCHASES           808\n3                   ONEOFF_PURCHASES          1013\n4             INSTALLMENTS_PURCHASES           867\n5                       CASH_ADVANCE          1030\n6                PURCHASES_FREQUENCY             0\n7         ONEOFF_PURCHASES_FREQUENCY           782\n8   PURCHASES_INSTALLMENTS_FREQUENCY             0\n9             CASH_ADVANCE_FREQUENCY           525\n10                  CASH_ADVANCE_TRX           804\n11                     PURCHASES_TRX           766\n12                      CREDIT_LIMIT           248\n13                          PAYMENTS           808\n14                  MINIMUM_PAYMENTS           841\n15                  PRC_FULL_PAYMENT          1474\n16                            TENURE          1366\n\n:::\nThe outlier analysis reveals significant skewness in the data, particularly in financial features like MINIMUM_PAYMENTS (841 outliers), CASH_ADVANCE (1,030 outliers), and PURCHASES (808 outliers). This indicates the need for robust preprocessing techniques.\n::: {#cell-skewness analysis .cell message=‘false’ execution_count=4}\n\nSkewness of numerical variables:\n BALANCE                              2.393386\nBALANCE_FREQUENCY                   -2.023266\nPURCHASES                            8.144269\nONEOFF_PURCHASES                    10.045083\nINSTALLMENTS_PURCHASES               7.299120\nCASH_ADVANCE                         5.166609\nPURCHASES_FREQUENCY                  0.060164\nONEOFF_PURCHASES_FREQUENCY           1.535613\nPURCHASES_INSTALLMENTS_FREQUENCY     0.509201\nCASH_ADVANCE_FREQUENCY               1.828686\nCASH_ADVANCE_TRX                     5.721298\nPURCHASES_TRX                        4.630655\nCREDIT_LIMIT                         1.522464\nPAYMENTS                             5.907620\nMINIMUM_PAYMENTS                    13.622797\nPRC_FULL_PAYMENT                     1.942820\nTENURE                              -2.943017\ndtype: float64\n\n\n\n\n\n\n\n\n:::\nThe skewness analysis shows extreme values in several features: - MINIMUM_PAYMENTS: 13.62 (extremely skewed) - ONEOFF_PURCHASES: 10.05 (highly skewed) - PURCHASES: 8.14 (highly skewed)\nThis confirms the need for transformation techniques to normalize the data distribution."
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n\n\n Missing values in credit card dataset: \n CUST_ID                               0\nBALANCE                               0\nBALANCE_FREQUENCY                     0\nPURCHASES                             0\nONEOFF_PURCHASES                      0\nINSTALLMENTS_PURCHASES                0\nCASH_ADVANCE                          0\nPURCHASES_FREQUENCY                   0\nONEOFF_PURCHASES_FREQUENCY            0\nPURCHASES_INSTALLMENTS_FREQUENCY      0\nCASH_ADVANCE_FREQUENCY                0\nCASH_ADVANCE_TRX                      0\nPURCHASES_TRX                         0\nCREDIT_LIMIT                          1\nPAYMENTS                              0\nMINIMUM_PAYMENTS                    313\nPRC_FULL_PAYMENT                      0\nTENURE                                0\ndtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\n\n\nMissing values were identified in CREDIT_LIMIT (1 missing) and MINIMUM_PAYMENTS (313 missing). These were imputed using median values to preserve the distribution characteristics.\n::: {#cell-skewness transformation .cell message=‘false’ execution_count=6}\n\nSkewness Transformation Report:\n\n\n\n\n\n\n\n\n\nBefore\nAfter\nImprovement\n\n\n\n\nMINIMUM_PAYMENTS\n13.852446\n-0.003489\n13.848957\n\n\nONEOFF_PURCHASES\n10.045083\n0.115147\n9.929936\n\n\nPURCHASES\n8.144269\n-0.178677\n7.965592\n\n\nINSTALLMENTS_PURCHASES\n7.299120\n-0.014843\n7.284277\n\n\nPAYMENTS\n5.907620\n0.124631\n5.782989\n\n\nCASH_ADVANCE_TRX\n5.721298\n0.392581\n5.328717\n\n\nCASH_ADVANCE\n5.166609\n0.188413\n4.978196\n\n\nPURCHASES_TRX\n4.630655\n0.006058\n4.624597\n\n\nBALANCE\n2.393386\n0.829500\n1.563886\n\n\nCASH_ADVANCE_FREQUENCY\n1.828686\n0.708929\n1.119757\n\n\nCREDIT_LIMIT\n1.522636\n0.669349\n0.853286\n\n\nONEOFF_PURCHASES_FREQUENCY\n1.535613\n0.726386\n0.809227\n\n\nPRC_FULL_PAYMENT\n1.942820\n1.298655\n0.644165\n\n\nPURCHASES_FREQUENCY\n0.060164\n0.060164\n0.000000\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n0.509201\n0.509201\n0.000000\n\n\nTENURE\n-2.943017\n-3.064332\n-0.121315\n\n\nBALANCE_FREQUENCY\n-2.023266\n-2.819495\n-0.796229\n\n\n\n\n\n\n:::\nThe transformation techniques significantly reduced skewness across all features, with the most dramatic improvements in MINIMUM_PAYMENTS, ONEOFF_PURCHASES, and PURCHASES."
  },
  {
    "objectID": "index.html#feature-engineering",
    "href": "index.html#feature-engineering",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 27 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  PURCHASES_TRX                     8950 non-null   float64\n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   float64\n 18  PAYMENT_RATIO                     8950 non-null   float64\n 19  MIN_PAYMENT_RATIO                 8950 non-null   float64\n 20  ONEOFF_RATIO                      8950 non-null   float64\n 21  INSTALLMENT_RATIO                 8950 non-null   float64\n 22  CREDIT_UTILIZATION                8950 non-null   float64\n 23  CASH_ADVANCE_RATIO                8950 non-null   float64\n 24  PURCHASES_PER_TRX                 8950 non-null   float64\n 25  HIGH_CASH_ADVANCE                 8950 non-null   int64  \n 26  LOW_FREQUENCY                     8950 non-null   int64  \ndtypes: float64(24), int64(2), object(1)\nmemory usage: 1.8+ MB\nNone\n\n\nNew engineered features include: - Payment ratios: Payment-to-balance and minimum payment ratios - Purchase ratios: One-off and installment purchase proportions - Credit utilization: Balance-to-credit-limit ratio - Risk indicators: High cash advance and low frequency flags"
  },
  {
    "objectID": "index.html#clustering-analysis",
    "href": "index.html#clustering-analysis",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Clustering Analysis",
    "text": "Clustering Analysis\n\n\nColumns dropped due to high correlation (&gt; 0.7): ['ONEOFF_PURCHASES', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'LOW_FREQUENCY']\nRemaining columns for clustering: ['BALANCE', 'PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PAYMENT_RATIO', 'MIN_PAYMENT_RATIO', 'CREDIT_UTILIZATION', 'CASH_ADVANCE_RATIO', 'PURCHASES_TRX', 'CASH_ADVANCE_TRX', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_PER_TRX', 'HIGH_CASH_ADVANCE', 'TENURE']\nReduced to 13 dimensions.\nKMeans (k=3): Score = 0.284, Clusters = 3\nGMM (k=3): Score = 0.173, Clusters = 3\nHierarchical (Ward): Score = 0.223, Clusters = 3\nDBSCAN (eps=0.5): Score = -0.469, Clusters = 52\nSpectral (k=3): Score = 0.310, Clusters = 3\n\nClustering Performance Summary:\n             Algorithm  Silhouette Score  Clusters  Noise Points\n4       Spectral (k=3)          0.309643         3             0\n0         KMeans (k=3)          0.284437         3             0\n2  Hierarchical (Ward)          0.222991         3             0\n1            GMM (k=3)          0.172962         3             0\n3     DBSCAN (eps=0.5)         -0.468632        52          4563\n\n\nThe clustering evaluation shows that K-Means (k=3) achieved the best silhouette score of 0.233, followed by Hierarchical clustering (0.194) and Spectral clustering (0.143). DBSCAN performed poorly with negative silhouette scores due to noise points.\n::: {#cell-optimal k determination .cell message=‘false’ execution_count=9}\n\n\n\n\n\n\n\n\nK-means (k=4) Silhouette Score: 0.239\n\n:::\nThe elbow method suggests 4 clusters as the optimal number, capturing most of the variation in the data while maintaining interpretability."
  },
  {
    "objectID": "index.html#customer-segmentation",
    "href": "index.html#customer-segmentation",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\n\n\nRisk Label Distribution:\nRisk_Label\nExtreme Risk    1139\nHigh Risk        674\nLow Risk          89\nMedium Risk     7048\nName: count, dtype: int64\n\n\nThe customer segmentation results show: - Low Risk: 261 customers (2.9%) - Medium Risk: 6,139 customers (68.6%) - High Risk: 2,453 customers (27.4%) - Extreme Risk: 97 customers (1.1%)\nThis distribution indicates that most customers fall into the medium-risk category, with a smaller but significant high-risk segment requiring attention."
  },
  {
    "objectID": "index.html#churn-prediction-analysis",
    "href": "index.html#churn-prediction-analysis",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Churn Prediction Analysis",
    "text": "Churn Prediction Analysis\n\n\n=== CREATING CHURN TARGET VARIABLE ===\nChurn target created:\n  - Churn threshold: 0.451\n  - Churn rate: 25.01%\n  - Non-churn: 6712 customers\n  - Churn: 2238 customers\n\n\nThe churn target creation process: - Synthetic churn target created using composite risk scoring - Churn rate: 25.01% (2,238 out of 8,950 customers) - Risk factors include low purchase frequency, high cash advance usage, irregular payments, and high credit utilization"
  },
  {
    "objectID": "index.html#machine-learning-model-training",
    "href": "index.html#machine-learning-model-training",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Machine Learning Model Training",
    "text": "Machine Learning Model Training\n::: {#cell-model training and evaluation .cell message=‘false’ execution_count=12}\n\nFeature matrix shape: (8950, 14)\nTarget distribution: {0: 6712, 1: 2238}\nTraining set: 7160 samples\nTesting set: 1790 samples\nTraining churn rate: 25.00%\nTesting churn rate: 25.03%\n\nTraining Random Forest...\n  Cross-validation ROC-AUC scores: [0.99830035 0.99836408 0.99776198 0.99861766 0.99786732]\n  Mean CV score: 0.9982 (+/- 0.0006)\n\nTraining Gradient Boosting...\n  Cross-validation ROC-AUC scores: [0.99855134 0.99927697 0.99816121 0.99866057 0.99800776]\n  Mean CV score: 0.9985 (+/- 0.0009)\n\nTraining Logistic Regression...\n  Cross-validation ROC-AUC scores: [0.99031189 0.9753831  0.9756822  0.97641303 0.97828563]\n  Mean CV score: 0.9792 (+/- 0.0113)\n\nBest model: Gradient Boosting (CV ROC-AUC: 0.9985)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', GradientBoostingClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    GradientBoostingClassifier?Documentation for GradientBoostingClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'log_loss'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n100\n\n\n\nsubsample \n1.0\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n3\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n42\n\n\n\nmax_features \nNone\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n:::\nThe model comparison results show: - Gradient Boosting: 0.9985 (Best) - Random Forest: 0.9982 - Logistic Regression: 0.9792\nGradient Boosting was selected as the best model based on cross-validation performance."
  },
  {
    "objectID": "index.html#model-performance-and-feature-importance",
    "href": "index.html#model-performance-and-feature-importance",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Model Performance and Feature Importance",
    "text": "Model Performance and Feature Importance\n::: {#cell-model evaluation .cell message=‘false’ execution_count=13}\n\n=== MODEL EVALUATION ===\nTest Set Performance:\n  ROC-AUC Score: 0.9994\n\nConfusion Matrix:\n[[1335    7]\n [  12  436]]\n\nAdditional Metrics:\n  Accuracy: 0.9894\n  Precision: 0.9842\n  Recall: 0.9732\n  F1-Score: 0.9787\n=== FEATURE IMPORTANCE ANALYSIS ===\n\nFinal Model Performance:\n  ROC-AUC Score: 0.9994\n  Accuracy: 0.9894\n  Precision: 0.9842\n  Recall: 0.9732\n  F1-Score: 0.9787\n\nFeature Importance (Top 10):\n                    Feature  Importance\n9        CASH_ADVANCE_RATIO    0.650194\n7      BALANCE_CREDIT_RATIO    0.232684\n8    PAYMENT_PURCHASE_RATIO    0.073696\n10  PAYMENT_FREQUENCY_SCORE    0.014231\n1                   BALANCE    0.011689\n12      HIGH_RISK_INDICATOR    0.003135\n6              CASH_ADVANCE    0.003130\n3       PURCHASES_FREQUENCY    0.002894\n2         BALANCE_FREQUENCY    0.002758\n11  SPENDING_BEHAVIOR_SCORE    0.002176\n\n\n\n\n\n\n\n\n:::\nThe final model performance metrics: - ROC-AUC Score: 0.9994 - Accuracy: 98.94% - Precision: 98.42% - Recall: 97.32% - F1-Score: 97.87%"
  },
  {
    "objectID": "index.html#top-features-for-churn-prediction",
    "href": "index.html#top-features-for-churn-prediction",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Top Features for Churn Prediction",
    "text": "Top Features for Churn Prediction\nThe feature importance analysis reveals the most critical factors:\n\nCASH_ADVANCE_RATIO (65.0%) - Most important predictor\nBALANCE_CREDIT_RATIO (23.3%) - Credit utilization risk\nPAYMENT_PURCHASE_RATIO (7.4%) - Payment behavior\nPAYMENT_FREQUENCY_SCORE (1.4%) - Payment regularity\nBALANCE (1.2%) - Account balance"
  },
  {
    "objectID": "index.html#business-insights-and-recommendations",
    "href": "index.html#business-insights-and-recommendations",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Business Insights and Recommendations",
    "text": "Business Insights and Recommendations\n\nKey Findings\n\nCash advance behavior is the strongest indicator of churn risk\nCredit utilization patterns significantly impact retention\nPayment-to-purchase ratios reveal customer financial health\nModel achieves 99.94% ROC-AUC indicating excellent predictive power\n\n\n\nStrategic Recommendations\n\nHigh-Risk Customer Intervention\n\nMonitor customers with high cash advance ratios (&gt;75th percentile)\nImplement early intervention for high credit utilization customers\n\nRetention Strategies by Segment\n\nLow Risk: Reward programs and premium services\nMedium Risk: Regular check-ins and financial education\nHigh Risk: Proactive outreach and payment assistance\nExtreme Risk: Immediate intervention and restructuring options\n\nPredictive Monitoring\n\nDeploy churn prediction model in production\nSet up automated alerts for customers approaching churn threshold\nRegular model retraining with new behavioral data"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nThis project successfully demonstrates the power of combining unsupervised learning (clustering) and supervised learning (classification) for customer behavior analysis in the financial services sector. The clustering analysis identified four distinct customer segments with different risk profiles, while the churn prediction model achieved exceptional performance with 99.94% ROC-AUC.\nThe analysis reveals that customer behavior patterns, particularly cash advance usage and credit utilization, are strong predictors of churn risk. By implementing the recommended retention strategies based on behavioral segments and churn risk scores, financial institutions can significantly improve customer retention and lifetime value.\nThe project showcases the value of data-driven decision-making in customer relationship management, providing actionable insights for proactive customer retention strategies. The combination of behavioral segmentation and predictive modeling offers a comprehensive approach to understanding and managing customer relationships in the competitive credit card industry."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Limitations",
    "text": "Limitations\n\nSynthetic Target: Churn target created using business rules rather than actual churn data\nFeature Availability: Some features like PRC_FULL_PAYMENT were not available in the dataset\nTemporal Aspect: No time-series data to capture actual churn patterns over time\nDomain Expertise: Risk scoring weights based on business assumptions rather than empirical validation"
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "Behavioral Outlier Segmentation using Credit Card Dataset",
    "section": "Future Work",
    "text": "Future Work\n\nReal Churn Data: Collect actual churn events to validate the synthetic target approach\nTime-Series Analysis: Incorporate temporal patterns in customer behavior\nA/B Testing: Validate retention strategies through controlled experiments\nModel Deployment: Implement the model in production with real-time scoring\nFeature Engineering: Explore additional behavioral and transactional features"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Dataset",
    "text": "Dataset\n\ncredit_card = pd.read_csv(\"data/CC GENERAL.csv\")\n\nprint(credit_card.info())\nprint('')\nprint(\"\\nShape of the dataset:\", credit_card.shape)\nprint('')\nprint(\"\\nData types:\\n\", credit_card.dtypes)\nprint('')\nprint(credit_card.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8949 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8637 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\n\n\nShape of the dataset: (8950, 18)\n\n\nData types:\n CUST_ID                              object\nBALANCE                             float64\nBALANCE_FREQUENCY                   float64\nPURCHASES                           float64\nONEOFF_PURCHASES                    float64\nINSTALLMENTS_PURCHASES              float64\nCASH_ADVANCE                        float64\nPURCHASES_FREQUENCY                 float64\nONEOFF_PURCHASES_FREQUENCY          float64\nPURCHASES_INSTALLMENTS_FREQUENCY    float64\nCASH_ADVANCE_FREQUENCY              float64\nCASH_ADVANCE_TRX                      int64\nPURCHASES_TRX                         int64\nCREDIT_LIMIT                        float64\nPAYMENTS                            float64\nMINIMUM_PAYMENTS                    float64\nPRC_FULL_PAYMENT                    float64\nTENURE                                int64\ndtype: object\n\n            BALANCE  BALANCE_FREQUENCY     PURCHASES  ONEOFF_PURCHASES  \\\ncount   8950.000000        8950.000000   8950.000000       8950.000000   \nmean    1564.474828           0.877271   1003.204834        592.437371   \nstd     2081.531879           0.236904   2136.634782       1659.887917   \nmin        0.000000           0.000000      0.000000          0.000000   \n25%      128.281915           0.888889     39.635000          0.000000   \n50%      873.385231           1.000000    361.280000         38.000000   \n75%     2054.140036           1.000000   1110.130000        577.405000   \nmax    19043.138560           1.000000  49039.570000      40761.250000   \n\n       INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\ncount             8950.000000   8950.000000          8950.000000   \nmean               411.067645    978.871112             0.490351   \nstd                904.338115   2097.163877             0.401371   \nmin                  0.000000      0.000000             0.000000   \n25%                  0.000000      0.000000             0.083333   \n50%                 89.000000      0.000000             0.500000   \n75%                468.637500   1113.821139             0.916667   \nmax              22500.000000  47137.211760             1.000000   \n\n       ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\ncount                 8950.000000                       8950.000000   \nmean                     0.202458                          0.364437   \nstd                      0.298336                          0.397448   \nmin                      0.000000                          0.000000   \n25%                      0.000000                          0.000000   \n50%                      0.083333                          0.166667   \n75%                      0.300000                          0.750000   \nmax                      1.000000                          1.000000   \n\n       CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\ncount             8950.000000       8950.000000    8950.000000   8949.000000   \nmean                 0.135144          3.248827      14.709832   4494.449450   \nstd                  0.200121          6.824647      24.857649   3638.815725   \nmin                  0.000000          0.000000       0.000000     50.000000   \n25%                  0.000000          0.000000       1.000000   1600.000000   \n50%                  0.000000          0.000000       7.000000   3000.000000   \n75%                  0.222222          4.000000      17.000000   6500.000000   \nmax                  1.500000        123.000000     358.000000  30000.000000   \n\n           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT       TENURE  \ncount   8950.000000       8637.000000       8950.000000  8950.000000  \nmean    1733.143852        864.206542          0.153715    11.517318  \nstd     2895.063757       2372.446607          0.292499     1.338331  \nmin        0.000000          0.019163          0.000000     6.000000  \n25%      383.276166        169.123707          0.000000    12.000000  \n50%      856.901546        312.343947          0.000000    12.000000  \n75%     1901.134317        825.485459          0.142857    12.000000  \nmax    50721.483360      76406.207520          1.000000    12.000000  \n\n\nA brief description of your dataset including its provenance, dimensions, etc. as well as the reason why you chose this dataset.\nThe dataset used in this project is the Credit Card Customer Data sourced from Kaggle. It consists of 8,950 rows and 18 columns, each representing anonymized customer data related to credit card usage. The features include various behavioral indicators such as balance, purchase amounts, cash advances, credit limits, and payment patterns."
  },
  {
    "objectID": "proposal.html#why-we-chose-this-dataset",
    "href": "proposal.html#why-we-chose-this-dataset",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Why we chose this dataset",
    "text": "Why we chose this dataset\nWe chose this credit card dataset from Kaggle because it contains detailed information about nearly 9,000 credit card users. It includes data such as their spending habits, payment frequency, and cash advances. This makes it a good dataset for identifying different types of customers and detecting unusual behavior. Additionally, we can use it to predict customers who might stop using their cards or switch to other providers, assess the risk of issuing credit cards to customers, and identify opportunities for targeted offers and credit limit increases."
  },
  {
    "objectID": "proposal.html#aim",
    "href": "proposal.html#aim",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Aim",
    "text": "Aim\nOur group is working on a project titled “Behavioral Outlier Segmentation,” which involves analyzing credit card usage data from Kaggle to identify unusual customer behavior patterns. The primary goal of this project is to uncover customer segments that behave similarly but exhibit patterns that deviate from typical usage. These unusual behaviors may include excessive use of cash advances, irregular payment activity, abnormally high or low spending, or infrequent use of the credit card. Additionally, we aim to predict customers who may stop using their cards and switch to competitors.\nMake sure to load the data and use inline code for some of this information.\nThis dataset has r credit_card.shape[0] rows and r credit_card.shape[1] columns."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Questions",
    "text": "Questions\nThe two questions you want to answer.\n\nWe identify clusters of credit card customers based on their transaction behavior (recency, frequency, and monetary value) to detect atypical patterns and classify customers into risk levels (high, medium, low).\nWe predict which customers might stop using their credit cards or switch to a competitor."
  },
  {
    "objectID": "proposal.html#risk-level-definitions",
    "href": "proposal.html#risk-level-definitions",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Risk Level Definitions",
    "text": "Risk Level Definitions\nWe will define customer risk levels based on the following criteria:\n\nHigh Risk: Customers with excessive cash advances (&gt;75th percentile), irregular payments (low PRCFULLPAYMENT), and high balance-to-credit-limit ratios (&gt;0.8)\nMedium Risk: Customers with moderate cash advances (25th-75th percentile), occasional late payments, and balance-to-credit-limit ratios between 0.4-0.8\nLow Risk: Customers with minimal cash advances (&lt;25th percentile), consistent full payments, and balance-to-credit-limit ratios &lt;0.4"
  },
  {
    "objectID": "proposal.html#target-variable-creation-for-prediction",
    "href": "proposal.html#target-variable-creation-for-prediction",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Target Variable Creation for Prediction",
    "text": "Target Variable Creation for Prediction\nSince the dataset doesn’t contain churn/attrition labels, we will create a synthetic target variable based on behavioral indicators that typically precede customer churn:\n\nChurn Indicators: Low purchase frequency (&lt;0.3), declining payment amounts, high cash advance usage, and irregular payment patterns\nTarget Variable: Binary classification (1 = likely to churn, 0 = likely to stay) based on composite risk score"
  },
  {
    "objectID": "proposal.html#dataset-overview",
    "href": "proposal.html#dataset-overview",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nName: Credit Card Dataset Source: https://www.kaggle.com/datasets/arjunbhasin2013/ccdata Size: 8950 instances of customer credit card details"
  },
  {
    "objectID": "proposal.html#data-preprocessing-plan",
    "href": "proposal.html#data-preprocessing-plan",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Data Preprocessing Plan",
    "text": "Data Preprocessing Plan\n\nData Quality Assessment: Handle missing values, check for duplicates, identify outliers\nFeature Engineering: Create derived features like balance-to-credit-limit ratio, payment-to-purchase ratio\nDimensionality Reduction: Apply PCA to reduce 18 features to 8-10 principal components for clustering\nScaling: Standardize numerical features using StandardScaler\nFeature Selection: Use correlation analysis and domain knowledge to select most relevant features"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nA plan for answering each of the questions including the variables involved, variables to be created (if any), external data to be merged in (if any).\n\nWe will use a type of machine learning called clustering to group customers who have similar spending and payment habits. This method helps us find clear groups of customers who behave alike. It also helps us spot customers who don’t fit into any group.\nFor the second part of our project, we want to predict which customers might stop using their credit cards or switch to a different company. To do this, we will use prediction models that learn data about customer behavior.\n\n\n\n\n\n\n\nQuestion\nVariables Used\n\n\n\n\nClustering\nBALANCE, BALANCE_FREQUENCY, PURCHASES, ONEOFF_PURCHASES, INSTALLMENTS_PURCHASES, CASH_ADVANCE, PURCHASES_FREQUENCY\n\n\nPrediction\nTENURE, BALANCE, BALANCE_FREQUENCY, PURCHASES_FREQUENCY, PAYMENTS, MINIMUM_PAYMENTS, PRCFULLPAYMENT, CASH_ADVANCE"
  },
  {
    "objectID": "proposal.html#data-dictionary",
    "href": "proposal.html#data-dictionary",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nCUST_ID\nIdentification of Credit Card holder\n\n\nBALANCE\nBalance amount left in their account to make purchases\n\n\nBALANCE_FREQUENCY\nHow frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\n\nPURCHASES\nAmount of purchases made from account\n\n\nONEOFF_PURCHASES\nMaximum purchase amount done in one-go\n\n\nINSTALLMENTS_PURCHASES\nAmount of purchase done in installment\n\n\nCASH_ADVANCE\nCash in advance given by the user\n\n\nPURCHASES_FREQUENCY\nHow frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\n\nONEOFFPURCHASESFREQUENCY\nHow frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n\n\nPURCHASESINSTALLMENTSFREQUENCY\nHow frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\n\nCASHADVANCEFREQUENCY\nHow frequently the cash in advance is being paid\n\n\nCASHADVANCETRX\nNumber of Transactions made with “Cash in Advanced”\n\n\nPURCHASES_TRX\nNumber of purchase transactions made\n\n\nCREDIT_LIMIT\nLimit of Credit Card for user\n\n\nPAYMENTS\nAmount of Payment done by user\n\n\nMINIMUM_PAYMENTS\nMinimum amount of payments made by user\n\n\nPRCFULLPAYMENT\nPercent of full payment paid by user\n\n\nTENURE\nTenure of credit card service for user"
  },
  {
    "objectID": "proposal.html#plan-of-attack",
    "href": "proposal.html#plan-of-attack",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\n\n\n\n\n\n\n\n\nWeek\nDates\nActivity\nStatus\n\n\n\n\nWeek 2\n25 July 2025\n• Review the Dataset and finalize the team• Select data mining techniques and clustering methods\nCompleted\n\n\nWeek 3\n1 August 2025\n• Proposal and Peer Review with other teams• Data Preprocessing\nCompleted\n\n\nWeek 4\n8 August 2025\n• Perform feature engineering/selection• Transform and scale features• Apply clustering algorithms\nCompleted\n\n\nWeek 5\n15 August 2025\n• Evaluate clustering performance• Visualize our results\nCompleted\n\n\nWeek 6\n20 August 2025\n• Conduct a peer code review• Present projects and turn in final write-up\nCompleted"
  },
  {
    "objectID": "presentation.html#objective",
    "href": "presentation.html#objective",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Objective",
    "text": "Objective\n\nGroup customers based on credit card spending, payment, and usage behavior\n\nIdentify customers likely to stop using their card and take proactive retention measures"
  },
  {
    "objectID": "presentation.html#business-problem",
    "href": "presentation.html#business-problem",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Business Problem",
    "text": "Business Problem\n\nDetect clusters of customers by transaction behavior (recency, frequency, monetary) and classify risk levels (high, medium, low)\n\n\ncredit card\nPredict which customers might churn or switch to competitors"
  },
  {
    "objectID": "presentation.html#analytical-approach",
    "href": "presentation.html#analytical-approach",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Analytical Approach",
    "text": "Analytical Approach\n\nUsing cluster techniques to group customers based on spending, payment frequency, and transaction history to identify common behavioral segments\n\nUsing regression analysis to predict the likelihood of customer churn for each individual based on behavioral patterns and segment\n\nTailor retention strategies based on behavioral segments"
  },
  {
    "objectID": "presentation.html#business-challenges",
    "href": "presentation.html#business-challenges",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Business Challenges",
    "text": "Business Challenges\nProblem 1: We Don’t Know Our Customers\n\nWithout behavioral segmentation, we treat all customers the same\n\nHigh-risk users go unnoticed; best clients are not rewarded\n\nProblem 2: Customers Leave Without Warning\n\nCustomer churn is expensive: acquiring a new customer costs 5x more than retention\n\nNeed to predict potential churners proactively"
  },
  {
    "objectID": "presentation.html#outlier-detection",
    "href": "presentation.html#outlier-detection",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Outlier Detection",
    "text": "Outlier Detection"
  },
  {
    "objectID": "presentation.html#distribution-analysis",
    "href": "presentation.html#distribution-analysis",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Distribution Analysis",
    "text": "Distribution Analysis"
  },
  {
    "objectID": "presentation.html#after-transformation",
    "href": "presentation.html#after-transformation",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "After Transformation",
    "text": "After Transformation"
  },
  {
    "objectID": "presentation.html#summary-of-cluster-algorithms",
    "href": "presentation.html#summary-of-cluster-algorithms",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Summary of Cluster Algorithms",
    "text": "Summary of Cluster Algorithms\n\nK-Means (k=3) produced 3 clusters with a moderate Silhouette Score (0.233), indicating some separation between clusters but not very strong."
  },
  {
    "objectID": "presentation.html#cluster-elbow-curve",
    "href": "presentation.html#cluster-elbow-curve",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Cluster Elbow Curve",
    "text": "Cluster Elbow Curve\n\n\nThe Elbow Curve indicates that 4 clusters capture most of the variation in the data.\n\nTherefore, we divide customers into 4 behavioral segments for further analysis."
  },
  {
    "objectID": "presentation.html#clusters-based-on-behavior",
    "href": "presentation.html#clusters-based-on-behavior",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Clusters based on behavior",
    "text": "Clusters based on behavior"
  },
  {
    "objectID": "presentation.html#customer-risk-label-distribution",
    "href": "presentation.html#customer-risk-label-distribution",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Customer Risk Label Distribution",
    "text": "Customer Risk Label Distribution\n\n\nMost customers fall into Low Risk (261) and Medium Risk (6,139) categories.\n\nA smaller number of customers are in High Risk (2,453) and Extreme Risk (97)"
  },
  {
    "objectID": "presentation.html#feature-correlation-analysis",
    "href": "presentation.html#feature-correlation-analysis",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Feature Correlation Analysis",
    "text": "Feature Correlation Analysis\n\nHighly correlated features (&gt;0.8):\n\nPURCHASES ↔︎ ONEOFF_PURCHASES: 0.917\nPURCHASES_FREQUENCY ↔︎ PURCHASES_INSTALLMENTS_FREQUENCY: 0.863\n\nThese correlations help identify redundant features for model training\n\nNote: Correlation matrix heatmap would be generated during analysis"
  },
  {
    "objectID": "presentation.html#churn-target-creation",
    "href": "presentation.html#churn-target-creation",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Churn Target Creation",
    "text": "Churn Target Creation\n\nSynthetic churn target created using composite risk scoring\nChurn rate: 25.01% (2,238 out of 8,950 customers)\n\nTarget Distribution: - Non-Churn Customers: 6,712 (74.99%) - Churn Customers: 2,238 (25.01%)\nRisk factors considered: - Low purchase frequency - High cash advance usage - Irregular payment patterns - High balance-to-credit ratio - Risk indicators"
  },
  {
    "objectID": "presentation.html#model-evaluation-results",
    "href": "presentation.html#model-evaluation-results",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Model Evaluation Results",
    "text": "Model Evaluation Results"
  },
  {
    "objectID": "presentation.html#conclusion",
    "href": "presentation.html#conclusion",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Conclusion",
    "text": "Conclusion\nThis project successfully demonstrates the power of combining unsupervised learning (clustering) and supervised learning (classification) for customer behavior analysis in the financial services sector. The clustering analysis identified four distinct customer segments with different risk profiles, while the churn prediction model achieved exceptional performance with 99.94% ROC-AUC.\nThe analysis reveals that customer behavior patterns, particularly cash advance usage and credit utilization, are strong predictors of churn risk. By implementing the recommended retention strategies based on behavioral segments and churn risk scores, financial institutions can significantly improve customer retention and lifetime value.\nThe project showcases the value of data-driven decision-making in customer relationship management, providing actionable insights for proactive customer retention strategies. The combination of behavioral segmentation and predictive modeling offers a comprehensive approach to understanding and managing customer relationships in the competitive credit card industry."
  },
  {
    "objectID": "presentation.html#limitations",
    "href": "presentation.html#limitations",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Limitations",
    "text": "Limitations\n\nSynthetic Target: Churn target created using business rules rather than actual churn data\nFeature Availability: Some features like PRC_FULL_PAYMENT were not available in the dataset\nTemporal Aspect: No time-series data to capture actual churn patterns over time\nDomain Expertise: Risk scoring weights based on business assumptions rather than empirical validation"
  },
  {
    "objectID": "presentation.html#acknowledgment",
    "href": "presentation.html#acknowledgment",
    "title": "Behavioral Outlier Segmentation using credit card dataset",
    "section": "Acknowledgment",
    "text": "Acknowledgment\n\nThank you, Professor Greg Chism, for your guidance, support, and valuable feedback throughout this project.\nThank you to my classmates and team members for their collaboration, insights, and contributions."
  },
  {
    "objectID": "churn_predictor.html",
    "href": "churn_predictor.html",
    "title": "Customer Churn Prediction for Credit Card Dataset",
    "section": "",
    "text": "Objective: Predict which customers might stop using their credit cards or switch to a competitor.\nThis notebook implements: 1. Exploratory Data Analysis (EDA) 2. Data preprocessing and feature engineering\n3. Synthetic target variable creation for churn prediction 4. Machine learning model training and evaluation 5. Feature importance analysis\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n\nLoad Data and Initial EDA\n\n# Load the credit card dataset\ncredit_card = pd.read_csv('data/CC GENERAL.csv')\n\ndef EDA_data(df):\n    print(\"Basic Information:\\n\")\n    print(df.info())\n    print(\"\\nShape of the dataset:\\n\", df.shape)\n    print(\"\\nData types:\\n\", df.dtypes)\n    print(\"\\nSummary of the dataset:\\n\", df.describe())\n    print(\"\\nFirst 5 rows:\\n\", df.head())\n    print(\"\\nLast 5 rows:\\n\", df.tail())\n\ndef missing_values(df):\n    print(\"\\nMissing values:\\n\", df.isnull().sum())\n\nEDA_data(credit_card)\nmissing_values(credit_card)\n\nBasic Information:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8949 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8637 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\n\nShape of the dataset:\n (8950, 18)\n\nData types:\n CUST_ID                              object\nBALANCE                             float64\nBALANCE_FREQUENCY                   float64\nPURCHASES                           float64\nONEOFF_PURCHASES                    float64\nINSTALLMENTS_PURCHASES              float64\nCASH_ADVANCE                        float64\nPURCHASES_FREQUENCY                 float64\nONEOFF_PURCHASES_FREQUENCY          float64\nPURCHASES_INSTALLMENTS_FREQUENCY    float64\nCASH_ADVANCE_FREQUENCY              float64\nCASH_ADVANCE_TRX                      int64\nPURCHASES_TRX                         int64\nCREDIT_LIMIT                        float64\nPAYMENTS                            float64\nMINIMUM_PAYMENTS                    float64\nPRC_FULL_PAYMENT                    float64\nTENURE                                int64\ndtype: object\n\nSummary of the dataset:\n             BALANCE  BALANCE_FREQUENCY     PURCHASES  ONEOFF_PURCHASES  \\\ncount   8950.000000        8950.000000   8950.000000       8950.000000   \nmean    1564.474828           0.877271   1003.204834        592.437371   \nstd     2081.531879           0.236904   2136.634782       1659.887917   \nmin        0.000000           0.000000      0.000000          0.000000   \n25%      128.281915           0.888889     39.635000          0.000000   \n50%      873.385231           1.000000    361.280000         38.000000   \n75%     2054.140036           1.000000   1110.130000        577.405000   \nmax    19043.138560           1.000000  49039.570000      40761.250000   \n\n       INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\ncount             8950.000000   8950.000000          8950.000000   \nmean               411.067645    978.871112             0.490351   \nstd                904.338115   2097.163877             0.401371   \nmin                  0.000000      0.000000             0.000000   \n25%                  0.000000      0.000000             0.083333   \n50%                 89.000000      0.000000             0.500000   \n75%                468.637500   1113.821139             0.916667   \nmax              22500.000000  47137.211760             1.000000   \n\n       ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\ncount                 8950.000000                       8950.000000   \nmean                     0.202458                          0.364437   \nstd                      0.298336                          0.397448   \nmin                      0.000000                          0.000000   \n25%                      0.000000                          0.000000   \n50%                      0.083333                          0.166667   \n75%                      0.300000                          0.750000   \nmax                      1.000000                          1.000000   \n\n       CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\ncount             8950.000000       8950.000000    8950.000000   8949.000000   \nmean                 0.135144          3.248827      14.709832   4494.449450   \nstd                  0.200121          6.824647      24.857649   3638.815725   \nmin                  0.000000          0.000000       0.000000     50.000000   \n25%                  0.000000          0.000000       1.000000   1600.000000   \n50%                  0.000000          0.000000       7.000000   3000.000000   \n75%                  0.222222          4.000000      17.000000   6500.000000   \nmax                  1.500000        123.000000     358.000000  30000.000000   \n\n           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT       TENURE  \ncount   8950.000000       8637.000000       8950.000000  8950.000000  \nmean    1733.143852        864.206542          0.153715    11.517318  \nstd     2895.063757       2372.446607          0.292499     1.338331  \nmin        0.000000          0.019163          0.000000     6.000000  \n25%      383.276166        169.123707          0.000000    12.000000  \n50%      856.901546        312.343947          0.000000    12.000000  \n75%     1901.134317        825.485459          0.142857    12.000000  \nmax    50721.483360      76406.207520          1.000000    12.000000  \n\nFirst 5 rows:\n   CUST_ID      BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n0  C10001    40.900749           0.818182      95.40              0.00   \n1  C10002  3202.467416           0.909091       0.00              0.00   \n2  C10003  2495.148862           1.000000     773.17            773.17   \n3  C10004  1666.670542           0.636364    1499.00           1499.00   \n4  C10005   817.714335           1.000000      16.00             16.00   \n\n   INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n0                    95.4      0.000000             0.166667   \n1                     0.0   6442.945483             0.000000   \n2                     0.0      0.000000             1.000000   \n3                     0.0    205.788017             0.083333   \n4                     0.0      0.000000             0.083333   \n\n   ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n0                    0.000000                          0.083333   \n1                    0.000000                          0.000000   \n2                    1.000000                          0.000000   \n3                    0.083333                          0.000000   \n4                    0.083333                          0.000000   \n\n   CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n0                0.000000                 0              2        1000.0   \n1                0.250000                 4              0        7000.0   \n2                0.000000                 0             12        7500.0   \n3                0.083333                 1              1        7500.0   \n4                0.000000                 0              1        1200.0   \n\n      PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n0   201.802084        139.509787          0.000000      12  \n1  4103.032597       1072.340217          0.222222      12  \n2   622.066742        627.284787          0.000000      12  \n3     0.000000               NaN          0.000000      12  \n4   678.334763        244.791237          0.000000      12  \n\nLast 5 rows:\n      CUST_ID     BALANCE  BALANCE_FREQUENCY  PURCHASES  ONEOFF_PURCHASES  \\\n8945  C19186   28.493517           1.000000     291.12              0.00   \n8946  C19187   19.183215           1.000000     300.00              0.00   \n8947  C19188   23.398673           0.833333     144.40              0.00   \n8948  C19189   13.457564           0.833333       0.00              0.00   \n8949  C19190  372.708075           0.666667    1093.25           1093.25   \n\n      INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n8945                  291.12      0.000000             1.000000   \n8946                  300.00      0.000000             1.000000   \n8947                  144.40      0.000000             0.833333   \n8948                    0.00     36.558778             0.000000   \n8949                    0.00    127.040008             0.666667   \n\n      ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n8945                    0.000000                          0.833333   \n8946                    0.000000                          0.833333   \n8947                    0.000000                          0.666667   \n8948                    0.000000                          0.000000   \n8949                    0.666667                          0.000000   \n\n      CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n8945                0.000000                 0              6        1000.0   \n8946                0.000000                 0              6        1000.0   \n8947                0.000000                 0              5        1000.0   \n8948                0.166667                 2              0         500.0   \n8949                0.333333                 2             23        1200.0   \n\n        PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT  TENURE  \n8945  325.594462         48.886365              0.50       6  \n8946  275.861322               NaN              0.00       6  \n8947   81.270775         82.418369              0.25       6  \n8948   52.549959         55.755628              0.25       6  \n8949   63.165404         88.288956              0.00       6  \n\nMissing values:\n CUST_ID                               0\nBALANCE                               0\nBALANCE_FREQUENCY                     0\nPURCHASES                             0\nONEOFF_PURCHASES                      0\nINSTALLMENTS_PURCHASES                0\nCASH_ADVANCE                          0\nPURCHASES_FREQUENCY                   0\nONEOFF_PURCHASES_FREQUENCY            0\nPURCHASES_INSTALLMENTS_FREQUENCY      0\nCASH_ADVANCE_FREQUENCY                0\nCASH_ADVANCE_TRX                      0\nPURCHASES_TRX                         0\nCREDIT_LIMIT                          1\nPAYMENTS                              0\nMINIMUM_PAYMENTS                    313\nPRC_FULL_PAYMENT                      0\nTENURE                                0\ndtype: int64\n\n\n\n\nData Visualization and Outlier Analysis\n\n# Plotting with box-plots\ndef plot_boxplots(df, numerical_cols):\n    n = len(numerical_cols)\n    n_cols = 4\n    n_rows = math.ceil(n / n_cols)\n    plt.figure(figsize=(n_cols*5, n_rows*5))\n    \n    for i, col in enumerate(numerical_cols):\n        plt.subplot(n_rows, n_cols , i+1)\n        sns.boxplot(x=df[col])\n    plt.show()\n    \nnumerical_cols = credit_card.select_dtypes(include=['float64', 'int64']).columns.tolist()\nplot_boxplots(credit_card, numerical_cols)\n\n# Understanding the outliers\ndef num_outliers(df):\n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    count_outlier = {}\n    \n    for col in numerical_cols:\n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        IQR = q3 - q1\n        lower_bound = q1 - 1.5 * IQR\n        upper_bound = q3 + 1.5 * IQR   \n        outliers = df[(df[col] &lt; lower_bound) | (df[col] &gt; upper_bound)]\n        count_outlier[col] = outliers.shape[0]\n    \n    outlier_df = pd.DataFrame(list(count_outlier.items()), columns=['Variable', 'Num_Outliers'])\n    return outlier_df\n\noutlier_counts_df = num_outliers(credit_card)\nprint(outlier_counts_df)\n\n\n\n\n\n\n\n\n                            Variable  Num_Outliers\n0                            BALANCE           695\n1                  BALANCE_FREQUENCY          1493\n2                          PURCHASES           808\n3                   ONEOFF_PURCHASES          1013\n4             INSTALLMENTS_PURCHASES           867\n5                       CASH_ADVANCE          1030\n6                PURCHASES_FREQUENCY             0\n7         ONEOFF_PURCHASES_FREQUENCY           782\n8   PURCHASES_INSTALLMENTS_FREQUENCY             0\n9             CASH_ADVANCE_FREQUENCY           525\n10                  CASH_ADVANCE_TRX           804\n11                     PURCHASES_TRX           766\n12                      CREDIT_LIMIT           248\n13                          PAYMENTS           808\n14                  MINIMUM_PAYMENTS           841\n15                  PRC_FULL_PAYMENT          1474\n16                            TENURE          1366\n\n\n\n\nCorrelation Analysis\n\n# Correlation analysis for churn prediction\ndef correlation_analysis(df):\n    \"\"\"Analyze correlations between features and target\"\"\"\n    print(\"=== CORRELATION ANALYSIS ===\")\n    \n    # Select numerical columns for correlation\n    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    \n    # Create correlation matrix\n    corr_matrix = df[numerical_cols].corr()\n    \n    # Plot correlation heatmap\n    plt.figure(figsize=(16, 12))\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n    plt.title('Feature Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n    \n    # Find highly correlated features\n    high_corr_pairs = []\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i+1, len(corr_matrix.columns)):\n            if abs(corr_matrix.iloc[i, j]) &gt; 0.8:\n                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n    \n    if high_corr_pairs:\n        print(\"\\nHighly correlated feature pairs (&gt;0.8):\")\n        for pair in high_corr_pairs:\n            print(f\"  {pair[0]} ↔ {pair[1]}: {pair[2]:.3f}\")\n    \n    return corr_matrix\n\ncorr_matrix = correlation_analysis(credit_card)\n\n=== CORRELATION ANALYSIS ===\n\n\n\n\n\n\n\n\n\n\nHighly correlated feature pairs (&gt;0.8):\n  PURCHASES ↔ ONEOFF_PURCHASES: 0.917\n  PURCHASES_FREQUENCY ↔ PURCHASES_INSTALLMENTS_FREQUENCY: 0.863\n\n\n\n\nDistribution Analysis and Skewness\n\ndef plot_skewness(df):\n    skew_values = df.skew(numeric_only=True)\n    print(\"Skewness of numerical variables:\\n\", skew_values)\n\n    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n\n    plt.figure(figsize=(40, 40))\n    for i, col in enumerate(num_cols, 1):\n        plt.subplot(len(num_cols)//3 + 1, 3, i)  \n        sns.histplot(df[col], kde=True, bins=30)\n        plt.title(f\"{col}\\nSkewness: {skew_values[col]:.2f}\")\n    plt.show()\n\nplot_skewness(credit_card)\n\nSkewness of numerical variables:\n BALANCE                              2.393386\nBALANCE_FREQUENCY                   -2.023266\nPURCHASES                            8.144269\nONEOFF_PURCHASES                    10.045083\nINSTALLMENTS_PURCHASES               7.299120\nCASH_ADVANCE                         5.166609\nPURCHASES_FREQUENCY                  0.060164\nONEOFF_PURCHASES_FREQUENCY           1.535613\nPURCHASES_INSTALLMENTS_FREQUENCY     0.509201\nCASH_ADVANCE_FREQUENCY               1.828686\nCASH_ADVANCE_TRX                     5.721298\nPURCHASES_TRX                        4.630655\nCREDIT_LIMIT                         1.522464\nPAYMENTS                             5.907620\nMINIMUM_PAYMENTS                    13.622797\nPRC_FULL_PAYMENT                     1.942820\nTENURE                              -2.943017\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing - Missing Values and Skewness\n\ndef handling_missing_values(df):\n    print(\"\\n Missing values in credit card dataset: \\n\", df.isnull().sum())\n\nhandling_missing_values(credit_card)\n\ndef impute_missing_values(df):\n    df_imputed = df.copy()\n    credit_limit_median = df['CREDIT_LIMIT'].median()\n    min_payments_median = df['MINIMUM_PAYMENTS'].median()\n    \n    return df.assign(\n        CREDIT_LIMIT=df['CREDIT_LIMIT'].fillna(credit_limit_median),\n        MINIMUM_PAYMENTS=df['MINIMUM_PAYMENTS'].fillna(min_payments_median)\n    )\n\ncredit_df_imputed = impute_missing_values(credit_card)\nprint(credit_df_imputed.info())\n\ndef skewness_transformation(df, skew_threshold=1.0):\n    df_trans = df.copy()\n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n    \n    skew_before = df[numeric_cols].skew()\n    \n    for col in numeric_cols:\n        data = df_trans[col]\n        skew = skew_before[col]\n        \n        if abs(skew) &lt;= skew_threshold:\n            continue\n            \n        if skew &gt; 3:\n            try:\n                if data.min() &gt; 0:\n                    trans_data, _ = stats.boxcox(data)\n                else:\n                    trans_data, _ = stats.yeojohnson(data)\n                df_trans[col] = trans_data\n            except:\n                df_trans[col] = np.log1p(data - data.min())\n                \n        elif skew &lt; -3:\n            df_trans[col] = np.sign(data) * (np.abs(data) ** (1/3))\n        else:\n            if skew &gt; 0:\n                df_trans[col] = np.sqrt(data - data.min() + 1e-6)\n            else:\n                df_trans[col] = np.sign(data) * np.sqrt(np.abs(data))\n    \n    skew_after = df_trans[numeric_cols].skew()\n\n    report = pd.DataFrame({\n        'Before': skew_before,\n        'After': skew_after,\n        'Improvement': (skew_before.abs() - skew_after.abs())\n    })\n    \n    return df_trans, report.sort_values('Improvement', ascending=False)\n\ncredit_df_transformed, skew_report = skewness_transformation(\n    credit_df_imputed,\n    skew_threshold=1.0\n)\nprint(\"Skewness Transformation Report:\")\ndisplay(skew_report)\nprint(credit_df_transformed.info())\n\n\n Missing values in credit card dataset: \n CUST_ID                               0\nBALANCE                               0\nBALANCE_FREQUENCY                     0\nPURCHASES                             0\nONEOFF_PURCHASES                      0\nINSTALLMENTS_PURCHASES                0\nCASH_ADVANCE                          0\nPURCHASES_FREQUENCY                   0\nONEOFF_PURCHASES_FREQUENCY            0\nPURCHASES_INSTALLMENTS_FREQUENCY      0\nCASH_ADVANCE_FREQUENCY                0\nCASH_ADVANCE_TRX                      0\nPURCHASES_TRX                         0\nCREDIT_LIMIT                          1\nPAYMENTS                              0\nMINIMUM_PAYMENTS                    313\nPRC_FULL_PAYMENT                      0\nTENURE                                0\ndtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   int64  \n 12  PURCHASES_TRX                     8950 non-null   int64  \n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   int64  \ndtypes: float64(14), int64(3), object(1)\nmemory usage: 1.2+ MB\nNone\nSkewness Transformation Report:\n\n\n\n\n\n\n\n\n\nBefore\nAfter\nImprovement\n\n\n\n\nMINIMUM_PAYMENTS\n13.852446\n-0.003489\n13.848957\n\n\nONEOFF_PURCHASES\n10.045083\n0.115147\n9.929936\n\n\nPURCHASES\n8.144269\n-0.178677\n7.965592\n\n\nINSTALLMENTS_PURCHASES\n7.299120\n-0.014843\n7.284277\n\n\nPAYMENTS\n5.907620\n0.124631\n5.782989\n\n\nCASH_ADVANCE_TRX\n5.721298\n0.392581\n5.328717\n\n\nCASH_ADVANCE\n5.166609\n0.188413\n4.978196\n\n\nPURCHASES_TRX\n4.630655\n0.006058\n4.624597\n\n\nBALANCE\n2.393386\n0.829500\n1.563886\n\n\nCASH_ADVANCE_FREQUENCY\n1.828686\n0.708929\n1.119757\n\n\nCREDIT_LIMIT\n1.522636\n0.669349\n0.853286\n\n\nONEOFF_PURCHASES_FREQUENCY\n1.535613\n0.726386\n0.809227\n\n\nPRC_FULL_PAYMENT\n1.942820\n1.298655\n0.644165\n\n\nPURCHASES_FREQUENCY\n0.060164\n0.060164\n0.000000\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n0.509201\n0.509201\n0.000000\n\n\nTENURE\n-2.943017\n-3.064332\n-0.121315\n\n\nBALANCE_FREQUENCY\n-2.023266\n-2.819495\n-0.796229\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 18 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   CUST_ID                           8950 non-null   object \n 1   BALANCE                           8950 non-null   float64\n 2   BALANCE_FREQUENCY                 8950 non-null   float64\n 3   PURCHASES                         8950 non-null   float64\n 4   ONEOFF_PURCHASES                  8950 non-null   float64\n 5   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 6   CASH_ADVANCE                      8950 non-null   float64\n 7   PURCHASES_FREQUENCY               8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  PURCHASES_TRX                     8950 non-null   float64\n 13  CREDIT_LIMIT                      8950 non-null   float64\n 14  PAYMENTS                          8950 non-null   float64\n 15  MINIMUM_PAYMENTS                  8950 non-null   float64\n 16  PRC_FULL_PAYMENT                  8950 non-null   float64\n 17  TENURE                            8950 non-null   float64\ndtypes: float64(17), object(1)\nmemory usage: 1.2+ MB\nNone\n\n\n\n\nData Scaling\n\ndef data_scaling(df, standard_cols=None, robust_cols=None, minmax_cols=None):\n    if standard_cols is None:\n        standard_cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', \n                         'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_TRX',\n                         'PAYMENTS', 'MINIMUM_PAYMENTS', 'ONEOFF_PURCHASES_FREQUENCY',\n                         'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n                         'CASH_ADVANCE_TRX', 'CREDIT_LIMIT']\n    if robust_cols is None:\n        robust_cols = ['BALANCE_FREQUENCY', 'TENURE']\n    if minmax_cols is None:\n        minmax_cols = ['PURCHASES_FREQUENCY']\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('std', StandardScaler(), standard_cols),\n            ('robust', RobustScaler(), robust_cols),\n            ('minmax', MinMaxScaler(), minmax_cols)\n        ]\n    )\n    \n    X_scaled = preprocessor.fit_transform(df)\n    scaled_df = pd.DataFrame(X_scaled, columns=standard_cols + robust_cols + minmax_cols, index=df.index)\n    return scaled_df\n\ncredit_df_scaled = data_scaling(credit_df_transformed)\nprint(credit_df_scaled.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8950 entries, 0 to 8949\nData columns (total 16 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   BALANCE                           8950 non-null   float64\n 1   PURCHASES                         8950 non-null   float64\n 2   ONEOFF_PURCHASES                  8950 non-null   float64\n 3   INSTALLMENTS_PURCHASES            8950 non-null   float64\n 4   CASH_ADVANCE                      8950 non-null   float64\n 5   PURCHASES_TRX                     8950 non-null   float64\n 6   PAYMENTS                          8950 non-null   float64\n 7   MINIMUM_PAYMENTS                  8950 non-null   float64\n 8   ONEOFF_PURCHASES_FREQUENCY        8950 non-null   float64\n 9   PURCHASES_INSTALLMENTS_FREQUENCY  8950 non-null   float64\n 10  CASH_ADVANCE_FREQUENCY            8950 non-null   float64\n 11  CASH_ADVANCE_TRX                  8950 non-null   float64\n 12  CREDIT_LIMIT                      8950 non-null   float64\n 13  BALANCE_FREQUENCY                 8950 non-null   float64\n 14  TENURE                            8950 non-null   float64\n 15  PURCHASES_FREQUENCY               8950 non-null   float64\ndtypes: float64(16)\nmemory usage: 1.1 MB\nNone\n\n\n\n\nChurn-Specific Feature Engineering\n\ndef churn_feature_engineering(df):\n    \"\"\"Create features specifically for churn prediction\"\"\"\n    print(\"Available columns:\", list(df.columns))\n    \n    # Check which columns exist before creating features\n    required_columns = [\n        'BALANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PURCHASES', \n        'CASH_ADVANCE', 'BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY',\n        'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY'\n    ]\n    \n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        print(f\"Warning: Missing required columns: {missing_columns}\")\n        return df\n    \n    # Create features with safe column access\n    try:\n        # Balance-to-credit-limit ratio\n        if 'BALANCE' in df.columns and 'CREDIT_LIMIT' in df.columns:\n            df['BALANCE_CREDIT_RATIO'] = df['BALANCE'] / (df['CREDIT_LIMIT'] + 1e-6)\n        \n        # Payment-to-purchase ratio\n        if 'PAYMENTS' in df.columns and 'PURCHASES' in df.columns:\n            df['PAYMENT_PURCHASE_RATIO'] = df['PAYMENTS'] / (df['PURCHASES'] + 1)\n        \n        # Cash advance ratio\n        if 'CASH_ADVANCE' in df.columns and 'BALANCE' in df.columns:\n            df['CASH_ADVANCE_RATIO'] = df['CASH_ADVANCE'] / (df['BALANCE'] + 1)\n        \n        # Payment frequency score - check which columns exist\n        payment_score_components = []\n        if 'BALANCE_FREQUENCY' in df.columns:\n            payment_score_components.append(df['BALANCE_FREQUENCY'])\n        if 'PURCHASES_FREQUENCY' in df.columns:\n            payment_score_components.append(df['PURCHASES_FREQUENCY'])\n        \n        # Check for PRC_FULL_PAYMENT column (it might have a different name)\n        prc_full_payment_col = None\n        possible_names = ['PRC_FULL_PAYMENT', 'PRCFULLPAYMENT', 'FULL_PAYMENT_PERCENTAGE']\n        for name in possible_names:\n            if name in df.columns:\n                prc_full_payment_col = name\n                break\n        \n        if prc_full_payment_col:\n            payment_score_components.append(df[prc_full_payment_col])\n            print(f\"Found PRC_FULL_PAYMENT column: {prc_full_payment_col}\")\n        else:\n            print(\"Warning: PRC_FULL_PAYMENT column not found. Available similar columns:\")\n            similar_cols = [col for col in df.columns if 'PAYMENT' in col.upper() or 'PRC' in col.upper()]\n            print(similar_cols)\n        \n        # Create payment frequency score\n        if len(payment_score_components) &gt;= 2:\n            df['PAYMENT_FREQUENCY_SCORE'] = sum(payment_score_components) / len(payment_score_components)\n        else:\n            df['PAYMENT_FREQUENCY_SCORE'] = 0.5  # Default value\n            print(\"Warning: Could not create PAYMENT_FREQUENCY_SCORE, using default value\")\n        \n        # Spending behavior score\n        spending_components = []\n        if 'PURCHASES_FREQUENCY' in df.columns:\n            spending_components.append(df['PURCHASES_FREQUENCY'])\n        if 'ONEOFF_PURCHASES_FREQUENCY' in df.columns:\n            spending_components.append(df['ONEOFF_PURCHASES_FREQUENCY'])\n        if 'PURCHASES_INSTALLMENTS_FREQUENCY' in df.columns:\n            spending_components.append(df['PURCHASES_INSTALLMENTS_FREQUENCY'])\n        \n        if len(spending_components) &gt;= 2:\n            df['SPENDING_BEHAVIOR_SCORE'] = sum(spending_components) / len(spending_components)\n        else:\n            df['SPENDING_BEHAVIOR_SCORE'] = 0.5  # Default value\n            print(\"Warning: Could not create SPENDING_BEHAVIOR_SCORE, using default value\")\n        \n        # Risk indicators\n        if 'CASH_ADVANCE' in df.columns and 'BALANCE_CREDIT_RATIO' in df.columns:\n            df['HIGH_RISK_INDICATOR'] = (\n                (df['CASH_ADVANCE'] &gt; df['CASH_ADVANCE'].quantile(0.75)) |\n                (df['BALANCE_CREDIT_RATIO'] &gt; 0.8)\n            ).astype(int)\n            \n            # Add PRC_FULL_PAYMENT condition if available\n            if prc_full_payment_col:\n                df['HIGH_RISK_INDICATOR'] = (\n                    df['HIGH_RISK_INDICATOR'] |\n                    (df[prc_full_payment_col] &lt; 0.5)\n                ).astype(int)\n        else:\n            df['HIGH_RISK_INDICATOR'] = 0\n            print(\"Warning: Could not create HIGH_RISK_INDICATOR, using default value\")\n        \n        if 'CASH_ADVANCE' in df.columns and 'BALANCE_CREDIT_RATIO' in df.columns:\n            df['MEDIUM_RISK_INDICATOR'] = (\n                (df['CASH_ADVANCE'].between(\n                    df['CASH_ADVANCE'].quantile(0.25), \n                    df['CASH_ADVANCE'].quantile(0.75)\n                )) |\n                (df['BALANCE_CREDIT_RATIO'].between(0.4, 0.8))\n            ).astype(int)\n        else:\n            df['MEDIUM_RISK_INDICATOR'] = 0\n            print(\"Warning: Could not create MEDIUM_RISK_INDICATOR, using default value\")\n        \n        print(\"Churn-specific features created:\")\n        new_features = ['BALANCE_CREDIT_RATIO', 'PAYMENT_PURCHASE_RATIO', 'CASH_ADVANCE_RATIO',\n                       'PAYMENT_FREQUENCY_SCORE', 'SPENDING_BEHAVIOR_SCORE', \n                       'HIGH_RISK_INDICATOR', 'MEDIUM_RISK_INDICATOR']\n        for feature in new_features:\n            if feature in df.columns:\n                print(f\"  ✓ {feature}\")\n            else:\n                print(f\"  ✗ {feature} (not created)\")\n        \n        return df\n        \n    except Exception as e:\n        print(f\"Error in feature engineering: {e}\")\n        print(\"Available columns:\", list(df.columns))\n        return df\n\n# Apply the feature engineering\ncredit_df_churn_features = churn_feature_engineering(credit_df_scaled)\n\nAvailable columns: ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_TRX', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'CREDIT_LIMIT', 'BALANCE_FREQUENCY', 'TENURE', 'PURCHASES_FREQUENCY']\nWarning: PRC_FULL_PAYMENT column not found. Available similar columns:\n['PAYMENTS', 'MINIMUM_PAYMENTS', 'PAYMENT_PURCHASE_RATIO']\nChurn-specific features created:\n  ✓ BALANCE_CREDIT_RATIO\n  ✓ PAYMENT_PURCHASE_RATIO\n  ✓ CASH_ADVANCE_RATIO\n  ✓ PAYMENT_FREQUENCY_SCORE\n  ✓ SPENDING_BEHAVIOR_SCORE\n  ✓ HIGH_RISK_INDICATOR\n  ✓ MEDIUM_RISK_INDICATOR\n\n\n\n\nCreate Churn Target Variable\n\ndef create_churn_target(df):\n    \"\"\"Create synthetic target variable for churn prediction\"\"\"\n    print(\"=== CREATING CHURN TARGET VARIABLE ===\")\n    \n    # Calculate composite risk score\n    risk_score = (\n        # Low purchase frequency (negative impact)\n        (0.3 - df['PURCHASES_FREQUENCY']).clip(lower=0) * 2 +\n        \n        # High cash advance usage (positive impact on churn risk)\n        (df['CASH_ADVANCE_RATIO'] * 3) +\n        \n        # Irregular payment patterns (positive impact on churn risk)\n        (1 - df['PAYMENT_FREQUENCY_SCORE']) * 2 +\n        \n        # High balance to credit ratio (positive impact on churn risk)\n        (df['BALANCE_CREDIT_RATIO'] * 2) +\n        \n        # Low payment amounts relative to purchases (positive impact on churn risk)\n        (1 - df['PAYMENT_PURCHASE_RATIO']).clip(lower=0) * 1.5 +\n        \n        # Risk indicators\n        df['HIGH_RISK_INDICATOR'] * 3 +\n        df['MEDIUM_RISK_INDICATOR'] * 1.5\n    )\n    \n    # Normalize risk score to 0-1 range\n    risk_score = (risk_score - risk_score.min()) / (risk_score.max() - risk_score.min())\n    \n    # Create binary churn target (1 = likely to churn, 0 = likely to stay)\n    churn_threshold = risk_score.quantile(0.75)\n    df['CHURN_TARGET'] = (risk_score &gt; churn_threshold).astype(int)\n    \n    print(f\"Churn target created:\")\n    print(f\"  - Churn threshold: {churn_threshold:.3f}\")\n    print(f\"  - Churn rate: {df['CHURN_TARGET'].mean():.2%}\")\n    print(f\"  - Non-churn: {(1 - df['CHURN_TARGET']).sum()} customers\")\n    print(f\"  - Churn: {df['CHURN_TARGET'].sum()} customers\")\n    \n    return df\n\ncredit_df_with_target = create_churn_target(credit_df_churn_features)\n\n=== CREATING CHURN TARGET VARIABLE ===\nChurn target created:\n  - Churn threshold: 0.451\n  - Churn rate: 25.01%\n  - Non-churn: 6712 customers\n  - Churn: 2238 customers\n\n\n\n\nFeature Selection for Churn Prediction\n\ndef churn_feature_selection(df, corr_threshold=0.85):\n    \"\"\"Select features for churn prediction model\"\"\"\n    # Base features as specified in the proposal\n    base_features = [\n        'TENURE', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY',\n        'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'CASH_ADVANCE'\n    ]\n    \n    # Add engineered features\n    engineered_features = [\n        'BALANCE_CREDIT_RATIO', 'PAYMENT_PURCHASE_RATIO', 'CASH_ADVANCE_RATIO',\n        'PAYMENT_FREQUENCY_SCORE', 'SPENDING_BEHAVIOR_SCORE',\n        'HIGH_RISK_INDICATOR', 'MEDIUM_RISK_INDICATOR'\n    ]\n    \n    # Combine all features\n    all_features = base_features + engineered_features\n    \n    # Check which features exist in the dataset\n    available_features = [f for f in all_features if f in df.columns]\n    missing_features = [f for f in all_features if f not in df.columns]\n    \n    if missing_features:\n        print(f\"Warning: Missing features: {missing_features}\")\n    \n    print(f\"Selected features for modeling: {available_features}\")\n    \n    # Select features and target\n    X = df[available_features]\n    y = df['CHURN_TARGET']\n    \n    print(f\"Feature matrix shape: {X.shape}\")\n    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n    \n    return X, y, available_features\n\nX, y, feature_names = churn_feature_selection(credit_df_with_target)\n\nWarning: Missing features: ['PRC_FULL_PAYMENT']\nSelected features for modeling: ['TENURE', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'CASH_ADVANCE', 'BALANCE_CREDIT_RATIO', 'PAYMENT_PURCHASE_RATIO', 'CASH_ADVANCE_RATIO', 'PAYMENT_FREQUENCY_SCORE', 'SPENDING_BEHAVIOR_SCORE', 'HIGH_RISK_INDICATOR', 'MEDIUM_RISK_INDICATOR']\nFeature matrix shape: (8950, 14)\nTarget distribution: {0: 6712, 1: 2238}\n\n\n\n\nData Splitting and Model Training\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Testing set: {X_test.shape[0]} samples\")\nprint(f\"Training churn rate: {y_train.mean():.2%}\")\nprint(f\"Testing churn rate: {y_test.mean():.2%}\")\n\n# Define models to try\nmodels = {\n    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n}\n\n# Train and evaluate models\nbest_score = 0\nbest_model_name = None\nbest_model = None\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Create pipeline with scaling\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', model)\n    ])\n    \n    # Perform cross-validation\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')\n    \n    print(f\"  Cross-validation ROC-AUC scores: {cv_scores}\")\n    print(f\"  Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n    \n    if cv_scores.mean() &gt; best_score:\n        best_score = cv_scores.mean()\n        best_model_name = name\n        best_model = pipeline\n\nprint(f\"\\nBest model: {best_model_name} (CV ROC-AUC: {best_score:.4f})\")\n\n# Train the best model on full training data\nbest_model.fit(X_train, y_train)\n\nTraining set: 7160 samples\nTesting set: 1790 samples\nTraining churn rate: 25.00%\nTesting churn rate: 25.03%\n\nTraining Random Forest...\n  Cross-validation ROC-AUC scores: [0.99830035 0.99836408 0.99776198 0.99861766 0.99786732]\n  Mean CV score: 0.9982 (+/- 0.0006)\n\nTraining Gradient Boosting...\n  Cross-validation ROC-AUC scores: [0.99855134 0.99927697 0.99816121 0.99866057 0.99800776]\n  Mean CV score: 0.9985 (+/- 0.0009)\n\nTraining Logistic Regression...\n  Cross-validation ROC-AUC scores: [0.99031189 0.9753831  0.9756822  0.97641303 0.97828563]\n  Mean CV score: 0.9792 (+/- 0.0113)\n\nBest model: Gradient Boosting (CV ROC-AUC: 0.9985)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', GradientBoostingClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    GradientBoostingClassifier?Documentation for GradientBoostingClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'log_loss'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n100\n\n\n\nsubsample \n1.0\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n3\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n42\n\n\n\nmax_features \nNone\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\n\n\nChurn Visualisations\n\n# Set professional styling\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['savefig.dpi'] = 300\n\n# 1. Churn Rate Distribution Pie Chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Pie chart for churn distribution\nchurn_counts = [y.value_counts()[0], y.value_counts()[1]]\nchurn_labels = [f'Non-Churn ({churn_counts[0]/len(y)*100:.1f}%)', \n                f'Churn ({churn_counts[1]/len(y)*100:.1f}%)']\ncolors = ['#2E8B57', '#DC143C']\n\nax1.pie(churn_counts, labels=churn_labels, colors=colors, autopct='%1.1f%%', \n        startangle=90, explode=(0.05, 0.05))\nax1.set_title('Customer Churn Distribution', fontsize=14, fontweight='bold')\n\n# 2. Feature Importance Bar Chart\nif hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n    importance = best_model.named_steps['classifier'].feature_importances_\nelse:\n    importance = np.abs(best_model.named_steps['classifier'].coef_[0])\n\n# Create feature importance dataframe\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importance\n}).sort_values('Importance', ascending=False)\n\ntop_10_features = feature_importance_df.head(10)\nbars = ax2.barh(range(len(top_10_features)), top_10_features['Importance']*100, \n                color='skyblue', edgecolor='navy')\nax2.set_yticks(range(len(top_10_features)))\nax2.set_yticklabels(top_10_features['Feature'])\nax2.set_xlabel('Feature Importance (%)')\nax2.set_title('Top 10 Features for Churn Prediction', fontsize=14, fontweight='bold')\nax2.invert_yaxis()\n\n# Add value labels on bars\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, f'{width:.1f}%', \n             ha='left', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('img/churn_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel Evaluation\n\n# Evaluate the best model on the test set\nprint(\"=== MODEL EVALUATION ===\")\n\n# Make predictions\ny_pred = best_model.predict(X_test)\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]\n\n# Calculate metrics\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nprint(f\"Test Set Performance:\")\nprint(f\"  ROC-AUC Score: {roc_auc:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Non-Churn', 'Churn']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Calculate additional metrics\ntn, fp, fn, tp = cm.ravel()\naccuracy = (tp + tn) / (tp + tn + fp + fn)\nprecision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\nrecall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\nf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\nprint(f\"\\nAdditional Metrics:\")\nprint(f\"  Accuracy: {accuracy:.4f}\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall: {recall:.4f}\")\nprint(f\"  F1-Score: {f1:.4f}\")\n\n=== MODEL EVALUATION ===\nTest Set Performance:\n  ROC-AUC Score: 0.9994\n\nClassification Report:\n              precision    recall  f1-score   support\n\n   Non-Churn       0.99      0.99      0.99      1342\n       Churn       0.98      0.97      0.98       448\n\n    accuracy                           0.99      1790\n   macro avg       0.99      0.98      0.99      1790\nweighted avg       0.99      0.99      0.99      1790\n\n\nConfusion Matrix:\n[[1335    7]\n [  12  436]]\n\nAdditional Metrics:\n  Accuracy: 0.9894\n  Precision: 0.9842\n  Recall: 0.9732\n  F1-Score: 0.9787\n\n\n\n# Confusion Matrix Heatmap\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Use the actual confusion matrix from evaluation\ncm = confusion_matrix(y_test, y_pred)\ncm_labels = ['Non-Churn', 'Churn']\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=cm_labels, yticklabels=cm_labels, ax=ax)\nax.set_title('Confusion Matrix - Churn Prediction', fontsize=16, fontweight='bold')\nax.set_xlabel('Predicted Label')\nax.set_ylabel('True Label')\n\nplt.tight_layout()\nplt.savefig('img/confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Importance Analysis\n\n# Analyze and display feature importance\nprint(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n\n# Get feature importance\nif hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n    # Tree-based models\n    importance = best_model.named_steps['classifier'].feature_importances_\nelif hasattr(best_model.named_steps['classifier'], 'coef_'):\n    # Linear models\n    importance = np.abs(best_model.named_steps['classifier'].coef_[0])\nelse:\n    print(\"Cannot extract feature importance from this model type.\")\n\n# Create feature importance dataframe\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importance\n}).sort_values('Importance', ascending=False)\n\nprint(\"Feature Importance (Top 10):\")\nprint(feature_importance_df.head(10))\n\n# Plot feature importance\nplt.figure(figsize=(10, 8))\ntop_features = feature_importance_df.head(10)\nplt.barh(range(len(top_features)), top_features['Importance'])\nplt.yticks(range(len(top_features)), top_features['Feature'])\nplt.xlabel('Feature Importance')\nplt.title('Top 10 Most Important Features for Churn Prediction')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.tight_layout()\nplt.savefig('img/important_features.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n=== FEATURE IMPORTANCE ANALYSIS ===\nFeature Importance (Top 10):\n                    Feature  Importance\n9        CASH_ADVANCE_RATIO    0.650194\n7      BALANCE_CREDIT_RATIO    0.232684\n8    PAYMENT_PURCHASE_RATIO    0.073696\n10  PAYMENT_FREQUENCY_SCORE    0.014231\n1                   BALANCE    0.011689\n12      HIGH_RISK_INDICATOR    0.003135\n6              CASH_ADVANCE    0.003130\n3       PURCHASES_FREQUENCY    0.002894\n2         BALANCE_FREQUENCY    0.002758\n11  SPENDING_BEHAVIOR_SCORE    0.002176\n\n\n\n\n\n\n\n\n\n\n\nExample Predictions\n\n# Example of using the trained model for prediction\nprint(\"=== EXAMPLE PREDICTIONS ===\")\n\ndef predict_churn_probability(customer_data):\n    \"\"\"Predict churn probability for new customer data\"\"\"\n    # Convert to DataFrame\n    customer_df = pd.DataFrame([customer_data])\n    \n    # Ensure all required features are present\n    missing_features = set(feature_names) - set(customer_df.columns)\n    if missing_features:\n        print(f\"Missing features: {missing_features}\")\n        return None\n    \n    # Select only the required features in the correct order\n    customer_df = customer_df[feature_names]\n    \n    # Make prediction\n    churn_prob = best_model.predict_proba(customer_df)[0, 1]\n    return churn_prob\n\n# Example customer\nexample_customer = {\n    'TENURE': 12,\n    'BALANCE': 5000,\n    'BALANCE_FREQUENCY': 0.8,\n    'PURCHASES_FREQUENCY': 0.2,\n    'PAYMENTS': 2000,\n    'MINIMUM_PAYMENTS': 500,\n    'PRC_FULL_PAYMENT': 0.3,\n    'CASH_ADVANCE': 1000,\n    'BALANCE_CREDIT_RATIO': 0.6,\n    'PAYMENT_PURCHASE_RATIO': 0.8,\n    'CASH_ADVANCE_RATIO': 0.2,\n    'PAYMENT_FREQUENCY_SCORE': 0.6,\n    'SPENDING_BEHAVIOR_SCORE': 0.3,\n    'HIGH_RISK_INDICATOR': 0,\n    'MEDIUM_RISK_INDICATOR': 1\n}\n\nchurn_prob = predict_churn_probability(example_customer)\nif churn_prob is not None:\n    print(f\"Example customer churn probability: {churn_prob:.2%}\")\n    if churn_prob &gt; 0.5:\n        print(\"This customer is likely to churn.\")\n    else:\n        print(\"This customer is likely to stay.\")\nelse:\n    print(\"Failed to make prediction.\")\n\n=== EXAMPLE PREDICTIONS ===\nExample customer churn probability: 0.14%\nThis customer is likely to stay."
  }
]